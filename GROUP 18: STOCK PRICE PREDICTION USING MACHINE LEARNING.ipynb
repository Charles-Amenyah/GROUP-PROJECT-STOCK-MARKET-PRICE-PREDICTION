{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3541929e-d97f-4154-a8ef-ddceafee8d52",
   "metadata": {},
   "source": [
    "# <center> **STRUCTURING AND IMPORTING THE NECESSARY LIBRARIES**  </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be61ebdf-61b2-4667-8d33-2fedb667a686",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'formatargspec' from 'inspect' (/Users/CHARLES/anaconda3/lib/python3.11/inspect.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Bi-LSTM For Machine Learning\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/__init__.py:47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     45\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cancellation\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_conversion_registry\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core \u001b[38;5;28;01mas\u001b[39;00m core_types\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nested_structure_coder\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trace\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/saved_model/nested_structure_coder.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m internal\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collections_abc\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest.py:95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_nest\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest_util\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     99\u001b[0m STRUCTURES_HAVE_MISMATCHING_LENGTHS \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    100\u001b[0m     nest_util\u001b[38;5;241m.\u001b[39mSTRUCTURES_HAVE_MISMATCHING_LENGTHS\n\u001b[1;32m    101\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_collections\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwrapt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_wrapt\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wrapt/__init__.py:10\u001b[0m\n\u001b[1;32m      2\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(__version_info__)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ObjectProxy, CallableObjectProxy, FunctionWrapper,\n\u001b[1;32m      5\u001b[0m         BoundFunctionWrapper, WeakFunctionProxy, PartialCallableObjectProxy,\n\u001b[1;32m      6\u001b[0m         resolve_path, apply_patch, wrap_object, wrap_object_attribute,\n\u001b[1;32m      7\u001b[0m         function_wrapper, wrap_function_wrapper, patch_function_wrapper,\n\u001b[1;32m      8\u001b[0m         transient_function_wrapper)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (adapter_factory, AdapterFactory, decorator,\n\u001b[1;32m     11\u001b[0m         synchronized)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (register_post_import_hook, when_imported,\n\u001b[1;32m     14\u001b[0m         notify_module_loaded, discover_post_import_hooks)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getcallargs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wrapt/decorators.py:34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m builtins\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ismethod, isclass, formatargspec\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lock, RLock\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'formatargspec' from 'inspect' (/Users/CHARLES/anaconda3/lib/python3.11/inspect.py)"
     ]
    }
   ],
   "source": [
    "#IMPORT ALL THE NECESSARY LIBRARIES \n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from matplotlib.transforms import Affine2D\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf , plot_pacf\n",
    "import mplcursors  # Import mplcursors for interactive plotting\n",
    "import itertools\n",
    "\n",
    "\n",
    "#We use the glob () function since we are working from the same directory \n",
    "import os\n",
    "import glob \n",
    "\n",
    "# Statistical Computation\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Prophet model for Time Series\n",
    "from prophet import Prophet\n",
    "\n",
    "# Random Forest and Linear Regression for Machine Learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Bi-LSTM For Machine Learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout\n",
    "import contextlib\n",
    "import sys\n",
    "\n",
    "# ARIMA For Time Series\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Training and Testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics Computation\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "#Importing Warnings and Hiding them\n",
    "import warnings                \n",
    "warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e57c9-fc17-42a5-860b-15722bc483e4",
   "metadata": {},
   "source": [
    "# <center>**IMPORTING AND REVIEWING THE DATASET**</center>\n",
    "#### <u>CLOSING PRICE FOR HSBC, BAC, BARC AND JPM</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e3b4d-2443-4a1b-804c-77e9d43b036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file with proper date parsing\n",
    "df_stock = pd.read_csv('stock-yy-mm-dd.csv', parse_dates=['Dates'])\n",
    "\n",
    "# Convert 'Dates' column to datetime format\n",
    "df_stock['Dates'] = pd.to_datetime(df_stock['Dates'], format='%Y-%m-%d')\n",
    "\n",
    "# Check the basic information about the DataFrame\n",
    "df_stock.info()\n",
    "\n",
    "# This line will print the first 1044 rows of the DataFrame\n",
    "df_stock.head(1044)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfbfcf-228c-4788-86cf-5d4d287ed448",
   "metadata": {},
   "source": [
    "#### <u>SCALING THE DATA</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb3418-e8ef-4654-9edb-2bb4149b9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select the columns to be scaled (excluding any non-numeric columns)\n",
    "numeric_cols = df_stock.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Fit the scaler on the data and transform it\n",
    "df_stock_scaled = df_stock.copy()  # Make a copy of the DataFrame to avoid modifying the original data\n",
    "df_stock_scaled[numeric_cols] = scaler.fit_transform(df_stock[numeric_cols])\n",
    "\n",
    "# Now df_stock_scaled contains the scaled version of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb1a2b3-5e90-4eb8-86e0-a6f4bc8a1800",
   "metadata": {},
   "source": [
    "# <center>**DATA EXPLORATORY ANALYSIS**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31756def-a75f-4adf-b9cc-ddbeccd96a8f",
   "metadata": {},
   "source": [
    "#### <u>PLOTTING THE TREND FOR THE DATASET TO IDENTIFY PATTERNS</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f99eb9-39cb-4683-84da-05737c683fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color map for each line\n",
    "color_map = {'HSBA LN Equity': 'blue', \n",
    "             'BARC LN Equity': 'green', \n",
    "             'JPM UN Equity': 'red', \n",
    "             'BAC UN Equity': 'orange'}\n",
    "\n",
    "fig = px.line(df_stock, x='Dates', y=['HSBA LN Equity', 'BARC LN Equity', 'JPM UN Equity', 'BAC UN Equity'], \n",
    "              title='Stock Price Prediction', color_discrete_map=color_map)\n",
    "fig.update_xaxes(rangeslider_visible=False, tickangle=90)  # Rotate x-axis ticks vertically\n",
    "\n",
    "# Adjust trend line thickness\n",
    "fig.update_traces(line=dict(width=1))\n",
    "\n",
    "# Add legend\n",
    "fig.update_layout(showlegend=True)\n",
    "\n",
    "# Center the title\n",
    "fig.update_layout(title=dict(x=0.5, y=0.9, xanchor='center', yanchor='top'))\n",
    "\n",
    "# Increase the graph size and adjust margins\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,  # Adjust width as needed\n",
    "    height=600,  # Adjust height as needed\n",
    "    margin=dict(l=50, r=50, t=100, b=50),  # Adjust margins as needed\n",
    ")\n",
    "\n",
    "# Set graph background color\n",
    "fig.update_layout(plot_bgcolor='white')\n",
    "\n",
    "# Add grid\n",
    "fig.update_layout(xaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),\n",
    "                  yaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'))\n",
    "\n",
    "# Enable zoom and pan\n",
    "fig.update_layout(\n",
    "    dragmode='zoom',  # Enable zooming\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96554905-0ece-46b2-a411-23242a28cbcb",
   "metadata": {},
   "source": [
    "#### <u>DECOMPOSING OUR DATASET USING TIME SERIES ANALYSIS</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9923b8-a7a6-4548-88f9-de79eaeff931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time series decomposition for each time series\n",
    "decomposition_results = {}\n",
    "for column in ['HSBA LN Equity', 'BARC LN Equity', 'JPM UN Equity', 'BAC UN Equity']:\n",
    "    result = seasonal_decompose(df_stock[column], model='additive', period=12)  # Assuming a seasonal period of 12\n",
    "    decomposition_results[column] = result\n",
    "\n",
    "# Extract components for each time series\n",
    "trend = {column: result.trend for column, result in decomposition_results.items()}\n",
    "seasonal = {column: result.seasonal for column, result in decomposition_results.items()}\n",
    "residual = {column: result.resid for column, result in decomposition_results.items()}\n",
    "\n",
    "# Define color map for each line\n",
    "color_map = {'HSBA LN Equity': 'blue', \n",
    "             'BARC LN Equity': 'green', \n",
    "             'JPM UN Equity': 'red', \n",
    "             'BAC UN Equity': 'orange'}\n",
    "\n",
    "# Plot original time series\n",
    "fig = px.line(df_stock, x='Dates', y=['HSBA LN Equity', 'BARC LN Equity', 'JPM UN Equity', 'BAC UN Equity'], \n",
    "              title='Stock Price Prediction Decomposition')\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=False, tickangle=90)  # Rotate x-axis ticks vertically\n",
    "\n",
    "# Adjust trend line thickness and color\n",
    "fig.update_traces(line=dict(width=1, color='black'), name='Original')\n",
    "\n",
    "# Add legend\n",
    "fig.update_layout(showlegend=True)\n",
    "\n",
    "# Center the title\n",
    "fig.update_layout(title=dict(x=0.5, y=0.9, xanchor='center', yanchor='top'))\n",
    "\n",
    "# Increase the graph size and adjust margins\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,  # Adjust width as needed\n",
    "    height=600,  # Adjust height as needed\n",
    "    margin=dict(l=50, r=50, t=100, b=50),  # Adjust margins as needed\n",
    ")\n",
    "\n",
    "# Set graph background color\n",
    "fig.update_layout(plot_bgcolor='white')\n",
    "\n",
    "# Add grid\n",
    "fig.update_layout(xaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),\n",
    "                  yaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'))\n",
    "\n",
    "# Enable zoom and pan\n",
    "fig.update_layout(\n",
    "    dragmode='zoom',  # Enable zooming\n",
    ")\n",
    "\n",
    "# Plot decomposed components with legend for each time series\n",
    "for column in ['HSBA LN Equity', 'BARC LN Equity', 'JPM UN Equity', 'BAC UN Equity']:\n",
    "    fig.add_trace(go.Scatter(x=df_stock['Dates'], y=trend[column], mode='lines', name=f'Trend - {column}', line=dict(color=color_map[column])))\n",
    "    fig.add_trace(go.Scatter(x=df_stock['Dates'], y=seasonal[column], mode='lines', name=f'Seasonal - {column}', line=dict(color=color_map[column])))\n",
    "    fig.add_trace(go.Scatter(x=df_stock['Dates'], y=residual[column], mode='lines', name=f'Residual - {column}', line=dict(color=color_map[column])))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837e3ca-be4a-415c-a105-f39104f226d0",
   "metadata": {},
   "source": [
    "#### <u>SEPARATING THE DECOMPOSITION FOR EACH BANK [HSBA, BARC, JPM AND BAC]</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08900da-9a1c-4808-8859-eb7d46a4ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color map for each line\n",
    "color_map = {'HSBA LN Equity': 'blue', \n",
    "             'BARC LN Equity': 'green', \n",
    "             'JPM UN Equity': 'red', \n",
    "             'BAC UN Equity': 'orange'}\n",
    "\n",
    "# Plot original time series, trend, seasonality, and residual for each time series\n",
    "for column in ['HSBA LN Equity', 'BARC LN Equity', 'JPM UN Equity', 'BAC UN Equity']:\n",
    "    # Original Time Series\n",
    "    fig_original = go.Figure()\n",
    "    fig_original.add_trace(go.Scatter(x=df_stock['Dates'], y=df_stock[column], mode='lines', name=f'Original - {column}', line=dict(color=color_map[column])))\n",
    "    fig_original.update_layout(title=f'Original Time Series of {column}', xaxis_title='Dates', yaxis_title='Last Price')\n",
    "    fig_original.update_layout(plot_bgcolor='white', xaxis=dict(showgrid=True, gridwidth=1, gridcolor='black'), yaxis=dict(showgrid=True, gridwidth=1, gridcolor='black'))\n",
    "    fig_original.show()\n",
    "\n",
    "    # Trend Component\n",
    "    fig_trend = go.Figure()\n",
    "    fig_trend.add_trace(go.Scatter(x=df_stock['Dates'], y=trend[column], mode='lines', name=f'Trend - {column}', line=dict(color=color_map[column])))\n",
    "    fig_trend.update_layout(title=f'Trend Component of {column}', xaxis_title='Dates', yaxis_title='Last Price')\n",
    "    fig_trend.update_layout(plot_bgcolor='white', xaxis=dict(showgrid=True, gridwidth=1, gridcolor='black'), yaxis=dict(showgrid=True, gridwidth=1, gridcolor='black'))\n",
    "    fig_trend.show()\n",
    "\n",
    "    # Seasonal Component\n",
    "    fig_seasonal = go.Figure()\n",
    "    fig_seasonal.add_trace(go.Scatter(x=df_stock['Dates'], y=seasonal[column], mode='lines', name=f'Seasonal - {column}', line=dict(color=color_map[column])))\n",
    "    fig_seasonal.update_layout(title=f'Seasonal Component of {column}', xaxis_title='Dates', yaxis_title='Last Price')\n",
    "    fig_seasonal.update_layout(plot_bgcolor='white', xaxis=dict(showgrid=True, gridwidth=1, gridcolor='black'), yaxis=dict(showgrid=True, gridwidth=1, gridcolor='black'))\n",
    "    fig_seasonal.show()\n",
    "\n",
    "    # Residual Component\n",
    "    fig_residual = go.Figure()\n",
    "    fig_residual.add_trace(go.Scatter(x=df_stock['Dates'], y=residual[column], mode='lines', name=f'Residual - {column}', line=dict(color=color_map[column])))\n",
    "    fig_residual.update_layout(title=f'Residual Component of {column}', xaxis_title='Dates', yaxis_title='Last Price')\n",
    "    fig_residual.update_layout(plot_bgcolor='white', xaxis=dict(showgrid=True, gridwidth=1, gridcolor='black'), yaxis=dict(showgrid=True, gridwidth=1, gridcolor='black'))\n",
    "    fig_residual.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e8c72-17b2-4b77-b237-70d637ed5244",
   "metadata": {},
   "source": [
    "#### <u>CHECKING FOR OUTLIERS IN THE DATASET</u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba03ff-c878-46a0-9291-6ef5cc024446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create boxplots for each stock's closing price with switched axes\n",
    "ax = sns.boxplot(data=df_stock.iloc[:, 1:], orient=\"v\", palette=\"Set2\", showfliers=False)\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.title('Boxplot of Closing Prices for Each Stock')\n",
    "plt.xlabel('Stock')\n",
    "plt.ylabel('Closing Price')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcca860-6b2a-41a6-8d54-007fdd563b7b",
   "metadata": {},
   "source": [
    "#### <u>CHECKING FOR CORRELATION BETWEEN THE STOCKS [HSBA, BAC, BARC AND JPM]</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8349c5-9e8a-4325-8888-a4e08e8005ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = df_stock.iloc[:, 1:].corr()\n",
    "\n",
    "# Generate a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='viridis', fmt=\".2f\", annot_kws={\"size\": 10})\n",
    "plt.title('Correlation Matrix of Closing Prices')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c6ac6-3cea-4799-b0ee-8b76cfd4c6d3",
   "metadata": {},
   "source": [
    "#### <u>ANALYZING AUTOCORRELATION FOR HSBA, BACR, JPM AND BAC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186102af-631d-4bd4-b123-52e068190ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time series data\n",
    "time_series = df_stock[['HSBA LN Equity', 'BARC LN Equity', 'JPM UN Equity', 'BAC UN Equity']]  # Assuming these are the appropriate column names\n",
    "\n",
    "# Plot the autocorrelation function for each column\n",
    "fig, axes = plt.subplots(nrows=len(time_series.columns), figsize=(10, 6))\n",
    "\n",
    "for i, (column, series) in enumerate(time_series.items()):\n",
    "    plot_acf(series, lags=20, ax=axes[i])  # Adjust the number of lags as needed\n",
    "    axes[i].set_title(f'Autocorrelation Function for {column}')\n",
    "    axes[i].set_xlabel('Lag')\n",
    "    axes[i].set_ylabel('Autocorrelation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743cdd4b-70de-4aac-bcbe-23ce9b4e5f64",
   "metadata": {},
   "source": [
    "#### <u> CONDUCTING DICKEY-FULLER TEST ON STOCK TIME SERIES FOR STATIONARITY</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92a440-ddcb-46c1-b679-fa6e6b8f1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Dickey-Fuller test for each time series\n",
    "for column in ['HSBA LN Equity', 'BARC LN Equity', 'JPM UN Equity', 'BAC UN Equity']:\n",
    "    # Extract the time series data\n",
    "    time_series = df_stock[column]\n",
    "\n",
    "    # Perform the Dickey-Fuller test\n",
    "    result = adfuller(time_series)\n",
    "\n",
    "    # Print the test statistic and p-value\n",
    "    print(f'ADF Statistic for {column}:', result[0])\n",
    "    print(f'p-value for {column}:', result[1])\n",
    "\n",
    "    # Print critical values\n",
    "    print(f'Critical Values for {column}:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "    print()  # Add an empty line for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebaf0c-a63a-49d9-a292-6274e94f56dd",
   "metadata": {},
   "source": [
    "# <center>**STOCK PRICE PREDICTION WITH PROPHET**</center>\n",
    "### <center><u>**DATA PRE-PROCESSING**</u></center>\n",
    "#### <u>TRANSFORM THE DATASET INTO A MULTIPLE TIME SERIES MODEL DATASET</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4d48b-e884-4ef9-a231-fb82ffc2ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release Date from the index\n",
    "df_stock=df_stock.reset_index()\n",
    "\n",
    "# Melt the DataFrame to convert it to long format\n",
    "df_stock = pd.melt(df_stock, id_vars='Dates', value_vars=['HSBA LN Equity', 'BARC LN Equity', 'JPM UN Equity', 'BAC UN Equity'], var_name='ticker', value_name='y')\n",
    "df_stock.columns = ['ds', 'ticker', 'y']\n",
    "\n",
    "# Round the 'y' values to two decimal places\n",
    "df_stock['y'] = df_stock['y'].round(2)\n",
    "\n",
    "tickers_to_check = ['HSBA LN Equity', 'BARC LN Equity', 'JPM UN Equity', 'BAC UN Equity']\n",
    "\n",
    "#Check if the tickers are in the Data Frame\n",
    "for ticker in tickers_to_check:\n",
    "    if ticker in df_stock['ticker'].unique():\n",
    "        print(f\"{ticker} is present in the DataFrame.\")\n",
    "    else:\n",
    "        print(f\"{ticker} is not present in the DataFrame.\")\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df_stock.head(1043))\n",
    "df_stock.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2744701c-640d-462a-adba-7b0652d754f8",
   "metadata": {},
   "source": [
    "#### GROUPING THE DATA BY TICKER FOR BETTER ANALYSIS AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcea2f9-17a2-480e-8b3b-dd8895e937ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the data by ticker\n",
    "groups_by_ticker = df_stock.groupby('ticker')\n",
    "\n",
    "#Check the groups in the dataframe\n",
    "groups_by_ticker.groups.keys()\n",
    "\n",
    "# Check the shape of the grouped data\n",
    "group_shapes = {ticker: group_data.shape for ticker, group_data in groups_by_ticker}\n",
    "\n",
    "# Print the shapes\n",
    "for ticker, shape in group_shapes.items():\n",
    "    print(f\"Shape of data for {ticker}: {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac318c80-29fa-43cd-9ea1-b26e877ba014",
   "metadata": {},
   "source": [
    "# <center> **MODELING WITH PROPHET**</center>\n",
    "#### <u>SPLITTING THE DATA</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59023205-27d3-466b-a20a-26a918b923cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store the sum of rows for training and testing sets\n",
    "total_train_rows = 0\n",
    "total_test_rows = 0\n",
    "\n",
    "# Iterate over each ticker group\n",
    "for ticker, group_data in groups_by_ticker:\n",
    "    # Manually split the data for the current ticker group\n",
    "    split_index = int(len(group_data) * 0.8)  # 80% for training, 20% for testing\n",
    "    group_train = group_data[:split_index]  # Training subset includes data from the beginning up to the split index\n",
    "    group_test = group_data[split_index:]   # Testing subset includes data from the split index to the end\n",
    "    \n",
    "    # Add the number of rows in the training and testing sets to the total\n",
    "    total_train_rows += len(group_train)\n",
    "    total_test_rows += len(group_test)\n",
    "    \n",
    "    print(f\"Shape of data for {ticker}:\")\n",
    "    print(\"Training set:\", group_train.shape)\n",
    "    print(\"Testing set:\", group_test.shape)\n",
    "\n",
    "# Check if the sum of rows in the training and testing sets matches the total processed data shape\n",
    "if total_train_rows + total_test_rows == 4172:\n",
    "    print(\"The sum of rows in the training and testing sets for each ticker group tallies with the total processed data shape.\")\n",
    "else:\n",
    "    print(\"The sum of rows in the training and testing sets for each ticker group does not tally with the total processed data shape.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966eb37b-cab5-4c1e-9006-b337af17e696",
   "metadata": {},
   "source": [
    "#### <u>TRAINING AND FORECASTING</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527bb7c-6788-418b-8276-4f6fc5d9954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "def forecast_prophet(groups_by_ticker):\n",
    "    forecasts = []  # List to store forecasted results for each ticker group\n",
    "    observed_data = []  # List to store observed data for each ticker group\n",
    "    trained_models = {}  # Dictionary to store trained Prophet models for each ticker group\n",
    "\n",
    "    # Iterate over each ticker group\n",
    "    for ticker, group_data in groups_by_ticker:\n",
    "        # Split the data for the current ticker group\n",
    "        split_index = int(len(group_data) * 0.8)\n",
    "        group_train = group_data[:split_index]\n",
    "        group_test = group_data[split_index:]\n",
    "        \n",
    "        # Append both training and testing subsets to the observed data\n",
    "        observed_data.append((ticker, group_train, group_test))\n",
    "        \n",
    "        # Initialize Prophet model\n",
    "        model = Prophet()\n",
    "        \n",
    "        # Fit the model using only the training data\n",
    "        model.fit(group_train)\n",
    "\n",
    "        # Store the trained model in the dictionary\n",
    "        trained_models[ticker] = model\n",
    "\n",
    "        # Make predictions using the model for the next 365 days\n",
    "        future = model.make_future_dataframe(periods=365)\n",
    "        forecast = model.predict(future)[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "        forecast['ticker'] = ticker\n",
    "\n",
    "        # Store the forecasted results\n",
    "        forecasts.append(forecast[['ds', 'ticker', 'yhat', 'yhat_upper', 'yhat_lower']])\n",
    "    \n",
    "    # Concatenate forecasts for all ticker groups\n",
    "    all_forecasts = pd.concat(forecasts)\n",
    "\n",
    "    return all_forecasts, observed_data, trained_models\n",
    "\n",
    "# Call the function and pass the grouped data\n",
    "all_forecasts, observed_data, trained_models = forecast_prophet(groups_by_ticker)\n",
    "\n",
    "# Print the information about the DataFrame\n",
    "print(\"\\nINFORMATION ABOUT THE DATAFRAME:\")\n",
    "print(all_forecasts.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2dae4-5105-444b-8964-580caa18c93f",
   "metadata": {},
   "source": [
    "#### <u>REVIEWING OF THE SHAPE OF TRAIN AND TEST DATA TO SEE IF THERE ARE DISCREPANCIES</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9132c6-28b6-4896-bd43-0c095ddfbc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of the sizes of the training and testing sets for each ticker group\n",
    "total_train_size = sum([group_train.shape[0] for _, group_train, _ in observed_data])\n",
    "total_test_size = sum([group_test.shape[0] for _, _, group_test in observed_data])\n",
    "\n",
    "# Print the total sizes\n",
    "print(\"Total size of training sets:\", total_train_size)\n",
    "print(\"Total size of testing sets:\", total_test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5cbc8-2593-41bc-bebc-2dc3bb6e6d8f",
   "metadata": {},
   "source": [
    "# <center> **EVALUTING THE PERFORMANCE OF PROPHET**</center>\n",
    "#### <u>PROPHET PERFORMANCE BASED ON TICKERS</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f335754-f969-4a90-96a0-f3f8679f0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate MAPE and sMAPE\n",
    "def mape(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted)\n",
    "    return np.mean(np.abs((actual - predicted) / actual)) * 100 if np.any(actual) else float('inf')\n",
    "\n",
    "def smape(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted)\n",
    "    denominator = np.abs(actual) + np.abs(predicted)\n",
    "    return 2 * np.mean(np.abs(actual - predicted) / denominator) * 100 if np.any(denominator) else float('inf')\n",
    "\n",
    "# Initialize lists to store metrics for each ticker\n",
    "performance_metrics = []\n",
    "\n",
    "# Assume all_forecasts and observed_data are defined as mentioned earlier\n",
    "for ticker, group_train, group_test in observed_data:\n",
    "    # Merge forecast data with actual data based on date ('ds') and ticker\n",
    "    forecast = all_forecasts[all_forecasts['ticker'] == ticker]\n",
    "    actual_vs_predicted = pd.merge(group_test, forecast, on='ds', how='inner')\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    mse = mean_squared_error(actual_vs_predicted['y'], actual_vs_predicted['yhat'])\n",
    "    mae = mean_absolute_error(actual_vs_predicted['y'], actual_vs_predicted['yhat'])\n",
    "    r2 = r2_score(actual_vs_predicted['y'], actual_vs_predicted['yhat'])\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape_score = mape(actual_vs_predicted['y'], actual_vs_predicted['yhat'])\n",
    "    smape_score = smape(actual_vs_predicted['y'], actual_vs_predicted['yhat'])\n",
    "\n",
    "    # Store metrics in a dictionary for each ticker\n",
    "    performance_metrics.append({\n",
    "        'Ticker': ticker,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape_score,\n",
    "        'sMAPE': smape_score\n",
    "    })\n",
    "\n",
    "# Convert list of metrics to DataFrame for easier viewing\n",
    "metrics_df = pd.DataFrame(performance_metrics)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833c6f8-8b24-424a-bfc1-52cb1fe9a727",
   "metadata": {},
   "source": [
    "# <center>**VISUALIZING OUR FORECAST**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d456870-2afc-4dda-ab4c-b4b637af7c7e",
   "metadata": {},
   "source": [
    "#### <u>PLOTTING THE FORECAST WITH ACTUAL AND UNCERTAINTY INTERVALS BY TICKER</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946960d-e9fb-4c60-99ea-7ff7f22e5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Define distinct colors and settings for each type of data for each ticker\n",
    "colors = {\n",
    "    'HSBA LN Equity': {'train': 'navy', 'test': 'black', 'predicted': 'brown', 'interval': 'rgba(173, 216, 230, 0.4)'},\n",
    "    'BARC LN Equity': {'train': 'purple', 'test': 'red', 'predicted': 'green', 'interval': 'rgba(255, 182, 193, 0.4)'},\n",
    "    'JPM UN Equity': {'train': 'orange', 'test': 'green', 'predicted': 'red', 'interval': 'rgba(255, 99, 71, 0.4)'},\n",
    "    'BAC UN Equity': {'train': 'maroon', 'test': 'crimson', 'predicted': 'black', 'interval': 'rgba(240, 128, 128, 0.4)'}\n",
    "}\n",
    "\n",
    "# Assuming you have observed_data iterable with group_train, group_test per ticker\n",
    "for ticker, group_train, group_test in observed_data:\n",
    "    # Merge forecast data with actual data based on date ('ds') and ticker\n",
    "    forecast = all_forecasts[all_forecasts['ticker'] == ticker]\n",
    "    actual_vs_predicted = pd.merge(group_test, forecast, on='ds', how='inner')\n",
    "\n",
    "    # Add trace for training data as bubbles\n",
    "    fig.add_trace(go.Scatter(x=group_train['ds'], y=group_train['y'],\n",
    "                             mode='markers', name=f'{ticker} Train',\n",
    "                             marker=dict(size=5, color=colors[ticker]['train'])))\n",
    "\n",
    "    # Add trace for test data\n",
    "    fig.add_trace(go.Scatter(x=group_test['ds'], y=group_test['y'],\n",
    "                             mode='lines', name=f'{ticker} Test',\n",
    "                             line=dict(color=colors[ticker]['test'])))\n",
    "\n",
    "    # Add trace for predicted values\n",
    "    fig.add_trace(go.Scatter(x=actual_vs_predicted['ds'], y=actual_vs_predicted['yhat'],\n",
    "                             mode='lines', name=f'{ticker} Predicted',\n",
    "                             line=dict(color=colors[ticker]['predicted'])))\n",
    "\n",
    "    # Add uncertainty intervals with legend entry\n",
    "    interval_name = f'{ticker} Uncertainty Interval'\n",
    "    fig.add_trace(go.Scatter(x=actual_vs_predicted['ds'], y=actual_vs_predicted['yhat_upper'],\n",
    "                             mode='lines', name=interval_name,\n",
    "                             line=dict(width=0),\n",
    "                             showlegend=False))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=actual_vs_predicted['ds'], y=actual_vs_predicted['yhat_lower'],\n",
    "                             mode='lines', line=dict(width=0),\n",
    "                             fill='tonexty',  # Fill area between\n",
    "                             fillcolor=colors[ticker]['interval'],\n",
    "                             showlegend=True, legendgroup=interval_name, name=interval_name))\n",
    "\n",
    "    # Add a vertical line to indicate the start of test data for each ticker\n",
    "    fig.add_vline(x=group_test['ds'].iloc[0], line_width=1, line_dash=\"dash\", line_color=colors[ticker]['test'])\n",
    "\n",
    "# Update layout to enhance aesthetics and functionality\n",
    "fig.update_layout(\n",
    "    title='Training, Test, and Predicted Data with Uncertainty Intervals for Each Ticker',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Value',\n",
    "    legend_title=\"Legend\",\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(showgrid=True, gridcolor='lightgrey'),\n",
    "    yaxis=dict(showgrid=True, gridcolor='lightgrey'),\n",
    "    height=600,  # Adjust height based on your preference\n",
    "    width=1200,  # Adjust width based on your preference\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('TRAINING, TEST AND PREDICTION WITH UNCERTAINTY INTERVALS.png')  # Save the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e990b4-9f39-48f9-aa58-87e3716e4262",
   "metadata": {},
   "source": [
    "#### <u>SEPARATING THE PREDICTIONS WITH THE ACTUALS AND UNCERTAINTY LEVEL FOR EACH TICKER</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb028a-f300-4ea4-a52d-597f7e6a5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Define distinct colors and settings for each type of data for each ticker\n",
    "colors = {\n",
    "    'HSBA LN Equity': {'train': 'navy', 'test': 'black', 'predicted': 'brown', 'interval': 'rgba(173, 216, 230, 0.4)'},\n",
    "    'BARC LN Equity': {'train': 'purple', 'test': 'red', 'predicted': 'green', 'interval': 'rgba(255, 182, 193, 0.4)'},\n",
    "    'JPM UN Equity': {'train': 'orange', 'test': 'green', 'predicted': 'red', 'interval': 'rgba(255, 99, 71, 0.4)'},\n",
    "    'BAC UN Equity': {'train': 'maroon', 'test': 'crimson', 'predicted': 'black', 'interval': 'rgba(240, 128, 128, 0.4)'}\n",
    "}\n",
    "\n",
    "# Assuming you have observed_data iterable with group_train, group_test per ticker\n",
    "for ticker, group_train, group_test in observed_data:\n",
    "    # Initialize the figure for each ticker\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Merge forecast data with actual data based on date ('ds') and ticker\n",
    "    forecast = all_forecasts[all_forecasts['ticker'] == ticker]\n",
    "    actual_vs_predicted = pd.merge(group_test, forecast, on='ds', how='inner')\n",
    "\n",
    "    # Add trace for training data as bubbles\n",
    "    fig.add_trace(go.Scatter(x=group_train['ds'], y=group_train['y'],\n",
    "                             mode='markers', name='Train',\n",
    "                             marker=dict(size=5, color=colors[ticker]['train'])))\n",
    "\n",
    "    # Add trace for test data\n",
    "    fig.add_trace(go.Scatter(x=group_test['ds'], y=group_test['y'],\n",
    "                             mode='lines', name='Actual',\n",
    "                             line=dict(color=colors[ticker]['test'])))\n",
    "\n",
    "    # Add trace for predicted values\n",
    "    fig.add_trace(go.Scatter(x=actual_vs_predicted['ds'], y=actual_vs_predicted['yhat'],\n",
    "                             mode='lines', name='Predicted',\n",
    "                             line=dict(color=colors[ticker]['predicted'])))\n",
    "\n",
    "    # Add uncertainty intervals with legend entry\n",
    "    fig.add_trace(go.Scatter(x=actual_vs_predicted['ds'], y=actual_vs_predicted['yhat_upper'],\n",
    "                             mode='lines', line=dict(width=0),\n",
    "                             showlegend=False))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=actual_vs_predicted['ds'], y=actual_vs_predicted['yhat_lower'],\n",
    "                             mode='lines', line=dict(width=0),\n",
    "                             fill='tonexty',  # Fill area between\n",
    "                             fillcolor=colors[ticker]['interval'],\n",
    "                             showlegend=True, name='Uncertainty Interval'))\n",
    "\n",
    "    # Add a vertical line to indicate the start of test data for each ticker\n",
    "    fig.add_vline(x=group_test['ds'].iloc[0], line_width=1, line_dash=\"dash\", line_color=colors[ticker]['test'])\n",
    "\n",
    "    # Update layout to enhance aesthetics and functionality\n",
    "    fig.update_layout(\n",
    "        title=f'Training, Test, and Predicted Data with Uncertainty for {ticker}',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Value',\n",
    "        legend_title=\"Legend\",\n",
    "        plot_bgcolor='white',\n",
    "        xaxis=dict(showgrid=True, gridcolor='lightgrey'),\n",
    "        yaxis=dict(showgrid=True, gridcolor='lightgrey'),\n",
    "        height=600,  # Adjust height based on your preference\n",
    "        width=1200,  # Adjust width based on your preference\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig('TRAINING, TEST AND PREDICTION WITH UNCERTAINTY INTERVALS FOR ALL TICKERS.png')  # Save the figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cdc2f-ba0f-43af-8810-617047557d69",
   "metadata": {},
   "source": [
    "#### <u>PLOTTING THE PROPHET COMPONENT FOR TREND, SEASONALITY AND HOLIDAY USING THE FORECAST</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac736a81-a78f-4b97-9538-522e1cef93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique ticker symbols\n",
    "tickers = all_forecasts['ticker'].unique()\n",
    "\n",
    "# Plot the components for each ticker\n",
    "for ticker in tickers:\n",
    "    # Filter the forecasted data for the current ticker\n",
    "    forecast_ticker = all_forecasts[all_forecasts['ticker'] == ticker]\n",
    "    \n",
    "    # Initialize the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the trend component\n",
    "    plt.plot(forecast_ticker['ds'], forecast_ticker['yhat'], label='Trend', color='blue', alpha=0.7)\n",
    "    \n",
    "    # Plot the seasonal component\n",
    "    plt.plot(forecast_ticker['ds'], forecast_ticker['yhat_upper'], label='Seasonal', color='green', alpha=0.7)\n",
    "    \n",
    "    # Plot the holiday component\n",
    "    plt.plot(forecast_ticker['ds'], forecast_ticker['yhat_lower'], label='Holiday', color='red', alpha=0.7)\n",
    "    \n",
    "    # Set plot title and labels\n",
    "    plt.title(f'Components for Ticker: {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a22239-c3a3-43aa-8eba-c7897b8e1b25",
   "metadata": {},
   "source": [
    "# <center>**STOCK PRICE PREDICTION USING RANDOM FOREST**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c11a69-2bc1-450a-bc1e-4c58150d4c22",
   "metadata": {},
   "source": [
    "#### <u>Data Review</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15547783-49cc-4594-b515-0e2599a732fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BAC_STOCK_DATA.csv',parse_dates=['Dates'], encoding='utf-8')\n",
    "\n",
    "# Convert 'Dates' column to datetime format\n",
    "df['Dates'] = pd.to_datetime(df['Dates'], format='%d/%m/%Y')\n",
    "\n",
    "#print the first 144 row\n",
    "df.head(144)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066890bb-1a57-4a09-bfc4-149aab4426ad",
   "metadata": {},
   "source": [
    "# <center>**MODELING WITH RANDOM FOREST FOR BAC**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6234339-8738-420a-bc2f-aa4a8c6a48b2",
   "metadata": {},
   "source": [
    "#### <u>Splitting and Training</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2a31c-4524-4d29-a629-44c3fd1c8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df[['Dates']]\n",
    "y = df['BAC UN Equity']\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39446a11-91a1-4776-9f31-f7a45c881c4f",
   "metadata": {},
   "source": [
    "\n",
    "#### <u>RANDOM FOREST PERFORMANCE EVALUATION BASED ON BAC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4ef06-a426-4e94-b456-8a9ff3a6b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "explained_variance_train = explained_variance_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = mse_train ** 0.5\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate evaluation metrics on the validation set\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "explained_variance_val = explained_variance_score(y_val, y_val_pred)\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = mse_val ** 0.5\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "# Calculate evaluation metrics on the test set\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "explained_variance_test = explained_variance_score(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_train:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_train:.2f}\")\n",
    "print(f\"MAPE: {mape_train:.2f}\")\n",
    "print(f\"MSE: {mse_train:.2f}\")\n",
    "print(f\"RMSE: {rmse_train:.2f}\")\n",
    "print(f\"MAE: {mae_train:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_val:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_val:.2f}\")\n",
    "print(f\"MAPE: {mape_val:.2f}\")\n",
    "print(f\"MSE: {mse_val:.2f}\")\n",
    "print(f\"RMSE: {rmse_val:.2f}\")\n",
    "print(f\"MAE: {mae_val:.2f}\")\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_val:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_val:.2f}\")\n",
    "print(f\"MAPE: {mape_val:.2f}\")\n",
    "print(f\"MSE: {mse_val:.2f}\")\n",
    "print(f\"RMSE: {rmse_val:.2f}\")\n",
    "print(f\"MAE: {mae_val:.2f}\")\n",
    "\n",
    "# Calculate evaluation metrics on the validation set\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "explained_variance_val = explained_variance_score(y_val, y_val_pred)\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = mse_val ** 0.5\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_val:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_val:.2f}\")\n",
    "print(f\"MAPE: {mape_val:.2f}\")\n",
    "print(f\"MSE: {mse_val:.2f}\")\n",
    "print(f\"RMSE: {rmse_val:.2f}\")\n",
    "print(f\"MAE: {mae_val:.2f}\")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "\n",
    "print(f\"\\nR-squared: {r2_test:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_test:.2f}\")\n",
    "print(f\"MAPE: {mape_test:.2f}\")\n",
    "print(f\"MSE: {mse_test:.2f}\")\n",
    "print(f\"RMSE: {rmse_test:.2f}\")\n",
    "print(f\"MAE: {mae_test:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2927c-a10b-44f6-8eec-14c948d6c823",
   "metadata": {},
   "source": [
    "#### <u>VISUALIZING OUR FORECAST FOR BAC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0435d6f0-7065-4ce8-9ebc-6d1dbde5c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train['Dates'], y_train, label='Training Data')\n",
    "plt.scatter(X_test['Dates'], y_test, label='Test Data')\n",
    "plt.scatter(X_test['Dates'], y_test_pred, label='Predicted Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PX_LAST')\n",
    "plt.title('Random Forest Regression for BAC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ef5ad-3085-4cd8-9e36-12dfd0c65d28",
   "metadata": {},
   "source": [
    "#### <u>DATA REVIEW FOR BARC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b8941-f515-431c-923a-ae9748dabcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BARC.csv',parse_dates=['Dates'], encoding='utf-8')\n",
    "\n",
    "# Convert 'Dates' column to datetime format\n",
    "df['Dates'] = pd.to_datetime(df['Dates'], format='%d/%m/%Y')\n",
    "\n",
    "#print the first 144 row\n",
    "df.head(144)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b73dc2-6d3b-40b0-acd4-65c907166158",
   "metadata": {},
   "source": [
    "#### <u>SPLITTING AND TRAINING</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b6a9c-fc74-49b7-8f0a-e043f15d71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df[['Dates']]\n",
    "y = df['BARC LN Equity']\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4bb57-8ec1-4367-beda-35ccbad682bf",
   "metadata": {},
   "source": [
    "#### <u>RANDOM FOREST PERFORMANCE EVALUATION BASED ON BARC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe3d11-e55d-4852-bf31-7b33bee9dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "explained_variance_train = explained_variance_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = mse_train ** 0.5\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate evaluation metrics on the validation set\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "explained_variance_val = explained_variance_score(y_val, y_val_pred)\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = mse_val ** 0.5\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_val:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_val:.2f}\")\n",
    "print(f\"MAPE: {mape_val:.2f}\")\n",
    "print(f\"MSE: {mse_val:.2f}\")\n",
    "print(f\"RMSE: {rmse_val:.2f}\")\n",
    "print(f\"MAE: {mae_val:.2f}\")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "\n",
    "print(f\"\\nR-squared: {r2_test:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_test:.2f}\")\n",
    "print(f\"MAPE: {mape_test:.2f}\")\n",
    "print(f\"MSE: {mse_test:.2f}\")\n",
    "print(f\"RMSE: {rmse_test:.2f}\")\n",
    "print(f\"MAE: {mae_test:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f09b7-988b-4716-8ab4-bd38c7588a50",
   "metadata": {},
   "source": [
    "#### <u>VISUALIZING OUR FORECAST FOR BARC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e75cda-7b2e-4deb-9394-e51a23c6838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train['Dates'], y_train, label='Training Data')\n",
    "plt.scatter(X_test['Dates'], y_test, label='Test Data')\n",
    "plt.scatter(X_test['Dates'], y_test_pred, label='Predicted Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PX_LAST')\n",
    "plt.title('Random Forest Regression for BARC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108bf5c-033c-4b85-b8ea-1a3e5aa2d55a",
   "metadata": {},
   "source": [
    "#### <u>DATA REVIEW FOR HSBA</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8af4f0-bedc-497a-9a2e-0e3f2de41298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv(file):\n",
    "    data = pd.read_csv(file, skiprows=5)\n",
    "    return data\n",
    "    \n",
    "    # Convert 'Dates' column to datetime format\n",
    "df['Dates'] = pd.to_datetime(df['Dates'], format='%d/%m/%Y')\n",
    "\n",
    "file_path = 'hsbcbank.csv'\n",
    "df = import_csv(file_path)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be149b-d1ad-4ad2-9aeb-8da164ddd077",
   "metadata": {},
   "source": [
    "#### <u>Splitting and Training</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643527c2-8221-42eb-a48c-65c9c03980bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Dates' column to datetime format\n",
    "df['Dates'] = pd.to_datetime(df['Dates'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e53f0-d745-4710-a756-ce40e9850840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df[['Dates']]\n",
    "y = df['PX_LAST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f0496-d524-43b6-9285-24b60a373ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71516da6-850a-4a48-a0b0-eee846b4da6b",
   "metadata": {},
   "source": [
    "#### <u>RANDOM FOREST PERFORMANCE EVALUATION BASED ON HSBA</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4a5ea-404f-478a-b600-25b1150ba346",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "explained_variance_train = explained_variance_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = mse_train ** 0.5\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate evaluation metrics on the validation set\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "explained_variance_val = explained_variance_score(y_val, y_val_pred)\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = mse_val ** 0.5\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "# Calculate evaluation metrics on the test set\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "explained_variance_test = explained_variance_score(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_train:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_train:.2f}\")\n",
    "print(f\"MAPE: {mape_train:.2f}\")\n",
    "print(f\"MSE: {mse_train:.2f}\")\n",
    "print(f\"RMSE: {rmse_train:.2f}\")\n",
    "print(f\"MAE: {mae_train:.2f}\")\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_val:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_val:.2f}\")\n",
    "print(f\"MAPE: {mape_val:.2f}\")\n",
    "print(f\"MSE: {mse_val:.2f}\")\n",
    "print(f\"RMSE: {rmse_val:.2f}\")\n",
    "print(f\"MAE: {mae_val:.2f}\")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "\n",
    "print(f\"\\nR-squared: {r2_test:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_test:.2f}\")\n",
    "print(f\"MAPE: {mape_test:.2f}\")\n",
    "print(f\"MSE: {mse_test:.2f}\")\n",
    "print(f\"RMSE: {rmse_test:.2f}\")\n",
    "print(f\"MAE: {mae_test:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93e2d7-a79f-401d-a0b5-f2e40ecacecc",
   "metadata": {},
   "source": [
    "#### **VISUALIZING OUR FORECAST FOR HSBA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb81db-3eed-47dc-b16b-30b9903ad674",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train['Dates'], y_train, label='Training Data')\n",
    "plt.scatter(X_test['Dates'], y_test, label='Test Data')\n",
    "plt.scatter(X_test['Dates'], y_test_pred, label='Predicted Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PX_LAST')\n",
    "plt.title('Random Forest Regression for HSBC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af552b26-8362-4ce5-b412-d180651b7032",
   "metadata": {},
   "source": [
    "#### <u>Data Review For JPM</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11520f-ce39-4e35-bc43-d78ba7bc3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('JPM_STOCK_DATA.csv',parse_dates=['Dates'], encoding='utf-8')\n",
    "\n",
    "# Convert 'Dates' column to datetime format\n",
    "df['Dates'] = pd.to_datetime(df['Dates'], format='%d/%m/%Y')\n",
    "\n",
    "#print the first 144 row\n",
    "df.head(144)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4e57b-7f38-414d-805b-cb53d1da9c27",
   "metadata": {},
   "source": [
    "#### <u>Splitting and Training</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ae87a-1478-4d0e-b4c7-c1e5fcb8b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df[['Dates']]\n",
    "y = df['JPM UN Equity']\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c92bc-70b7-493d-a4c5-752f5e29cbe6",
   "metadata": {},
   "source": [
    "#### <u>RANDOM FOREST PERFORMANCE EVALUATION BASED ON JPM</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c657a3-c454-43e5-8988-b90e09298036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_pred = rf_model.predict(X_train) \n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "explained_variance_train = explained_variance_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = mse_train ** 0.5\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate evaluation metrics on the validation set\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "explained_variance_val = explained_variance_score(y_val, y_val_pred)\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = mse_val ** 0.5\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "# Calculate evaluation metrics on the test set\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "explained_variance_test = explained_variance_score(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_train:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_train:.2f}\")\n",
    "print(f\"MAPE: {mape_train:.2f}\")\n",
    "print(f\"MSE: {mse_train:.2f}\")\n",
    "print(f\"RMSE: {rmse_train:.2f}\")\n",
    "print(f\"MAE: {mae_train:.2f}\")\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_val:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_val:.2f}\")\n",
    "print(f\"MAPE: {mape_val:.2f}\")\n",
    "print(f\"MSE: {mse_val:.2f}\")\n",
    "print(f\"RMSE: {rmse_val:.2f}\")\n",
    "print(f\"MAE: {mae_val:.2f}\")\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_val:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_val:.2f}\")\n",
    "print(f\"MAPE: {mape_val:.2f}\")\n",
    "print(f\"MSE: {mse_val:.2f}\")\n",
    "print(f\"RMSE: {rmse_val:.2f}\")\n",
    "print(f\"MAE: {mae_val:.2f}\")\n",
    "\n",
    "# Calculate evaluation metrics on the validation set\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "explained_variance_val = explained_variance_score(y_val, y_val_pred)\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = mse_val ** 0.5\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_val:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_val:.2f}\")\n",
    "print(f\"MAPE: {mape_val:.2f}\")\n",
    "print(f\"MSE: {mse_val:.2f}\")\n",
    "print(f\"RMSE: {rmse_val:.2f}\")\n",
    "print(f\"MAE: {mae_val:.2f}\")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "\n",
    "print(f\"\\nR-squared: {r2_test:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_test:.2f}\")\n",
    "print(f\"MAPE: {mape_test:.2f}\")\n",
    "print(f\"MSE: {mse_test:.2f}\")\n",
    "print(f\"RMSE: {rmse_test:.2f}\")\n",
    "print(f\"MAE: {mae_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3e384-1bd3-46d9-b0b4-e32d242fd265",
   "metadata": {},
   "source": [
    "#### **VISUALIZING OUR FORECAST FOR JPM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbef5d4-b5d6-4b33-a6d2-47d368b76c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train['Dates'], y_train, label='Training Data')\n",
    "plt.scatter(X_test['Dates'], y_test, label='Test Data')\n",
    "plt.scatter(X_test['Dates'], y_test_pred, label='Predicted Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PX_LAST')\n",
    "plt.title('Random Forest Regression for JPM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fdb185-97ef-4280-9821-2bfd26456b09",
   "metadata": {},
   "source": [
    "# <center>**STOCK PRICE PREDICTION USING LINEAR REGRESSION**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31b401-f812-4585-905f-48e3752090c0",
   "metadata": {},
   "source": [
    "#### <u>Reviewing of the Original Dataset For BAC.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb34de6-5583-45a0-b1fc-c46a88b6ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line reads the data from the test_csv.csv file using the read_csv function from the pandas library and stores it in the data variable as a pandas DataFrame.\n",
    "\n",
    "stock_price = pd.read_csv(\"Stockprice_four.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab3fb3-96b6-4831-ac5d-c6bc5a37cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to datetime format\n",
    "#This line converts the 'Dates' column in the DataFrame from string format to datetime format using the to_datetime function from pandas. \n",
    "#The format='%d/%m/%Y' argument specifies that the dates are in the 'DD/MM/YYYY' format.\n",
    "\n",
    "stock_price['Dates'] = pd.to_datetime(stock_price['Dates'], format='%d/%m/%Y')\n",
    "\n",
    "stock_price.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cee97d-1745-43c8-9243-f2628f0480e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date column as the index\n",
    "#This line sets the 'Dates' column as the index of the DataFrame using the set_index method from pandas.\n",
    "\n",
    "stock_price = stock_price.set_index('Dates')\n",
    "stock_price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ccd39-fb8a-4ed4-a288-8f237e9b2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datetime index to a numerical representation (number of days since a reference date)\n",
    "reference_date = stock_price.index.min()\n",
    "X = (stock_price.index - reference_date).days.values.reshape(-1, 1)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e84ef-f91b-4b73-8f38-1d2917bdb659",
   "metadata": {},
   "source": [
    "#### <u>Splitting and Training</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15304d9b-77bc-4872-be43-b70ad9f431ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "#This line assigns the 'BARC LN Equity' column from the DataFrame to the y variable, which represents the target variable (stock prices). \n",
    "#The values attribute is used to convert the pandas Series to a numpy array.\n",
    "\n",
    "y = stock_price['BAC UN EQUITY'].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "#This line splits the data into training and test sets using the train_test_split function from scikit-learn. \n",
    "#The X and y variables contain the feature matrix and target variable, respectively. \n",
    "#The test_size=0.2 argument specifies that 20% of the data should be used for testing, and the remaining 80% for training. \n",
    "#The random_state=42 argument ensures reproducibility by setting a fixed seed for the random number generator.\n",
    "#The function returns four variables: X_train, X_test, y_train, and y_test, which contain the feature matrices and target variables for the training and test sets, respectively.\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "tests_size = len(X)-train_size\n",
    "X_train, X_test = X[0:train_size],X[train_size:len(X)]\n",
    "y_train, y_test = y[0:train_size],y[train_size:len(X)]\n",
    "\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f69f7-bc22-4a39-9fb9-01a3b7e41c3c",
   "metadata": {},
   "source": [
    "#### <u>Training The Model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafdd84-63db-4e83-b743-1bb6a0cb31cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the linear regression model\n",
    "#These lines create an instance of the LinearRegression model from scikit-learn and fit the model to the training data (X_train, y_train) using the fit method.\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training, validation, and test sets\n",
    "#These lines use the fitted linear regression model to make predictions on the training and test sets using the predict method. \n",
    "#The predicted values for the training set are stored in y_train_pred, and the predicted values for the test set are stored in y_test_pred.\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cbb585-49f1-4a71-a204-5a59debd2398",
   "metadata": {},
   "source": [
    "#### <u>PERFORMANCE EVALUATION BASED ON BAC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57bcdf-0993-4583-b9fb-9ceb6dcfe72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics on the training set\n",
    "#These lines calculate various evaluation metrics for assessing the performance of the linear regression model on both the training and test sets. The metrics calculated are:\n",
    "#r2_score: R-squared score, which measures how well the model fits the data (ranges from 0 to 1, with 1 being a perfect fit).\n",
    "#explained_variance_score: Explained variance score, which represents the proportion of variance in the target variable that is explained by the model.\n",
    "#mean_absolute_percentage_error: Mean Absolute Percentage Error (MAPE), which measures the average absolute percentage difference between the predicted and actual values.\n",
    "#mean_squared_error: Mean Squared Error (MSE), which measures the average squared difference between the predicted and actual values.\n",
    "#np.sqrt(mse): Root Mean Squared Error (RMSE), which is the square root of the MSE and provides a measure of the typical magnitude of the prediction errors.\n",
    "#mean_absolute_error: Mean Absolute Error (MAE), which measures the average absolute difference between the predicted and actual values.\n",
    "#Each metric is calculated separately for the training and test sets using the corresponding functions from the sklearn.metrics module.\n",
    "\n",
    "\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "explained_variance_train = explained_variance_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = mse_train ** 0.5\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics on the test set\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "explained_variance_test = explained_variance_score(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "#These lines print the calculated evaluation metrics for both the training and test sets. The f-strings are used to format the output with the metric names and values.\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_train:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_train:.2f}\")\n",
    "print(f\"MAPE: {mape_train:.2f}\")\n",
    "print(f\"MSE: {mse_train:.2f}\")\n",
    "print(f\"RMSE: {rmse_train:.2f}\")\n",
    "print(f\"MAE: {mae_train:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "\n",
    "print(f\"\\nR-squared: {r2_test:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_test:.2f}\")\n",
    "print(f\"MAPE: {mape_test:.2f}\")\n",
    "print(f\"MSE: {mse_test:.2f}\")\n",
    "print(f\"RMSE: {rmse_test:.2f}\")\n",
    "print(f\"MAE: {mae_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e1404-ccde-4d42-a82c-219f51d58703",
   "metadata": {},
   "source": [
    "#### **VISUALIZING OUR FORECAST FOR BAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa64a4a-dc4d-4e3d-9ef5-f1075c169f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test data with the trained model\n",
    "#These lines create a plot using Matplotlib to visualize the training and test data along with the trained linear regression model:\n",
    "\n",
    "#plt.figure(figsize=(12, 6)) creates a new figure with a specific size (12 inches wide and 6 inches tall).\n",
    "#plt.scatter(X_train, y_train, label='Training Data', alpha=0.3) plots the training data as scattered points, with a label and transparency level (alpha=0.3).\n",
    "#plt.scatter(X_test, y_test, label='Test Data', alpha=0.3) plots the test data as scattered points, with a label and transparency level (alpha=0.3).\n",
    "#plt.plot(X, model.predict(X), 'r', label='Trained Model') plots the trained linear regression model as a red line, using the predict method to obtain the predicted values for the entire feature matrix X.\n",
    "#plt.xlabel('Date') and plt.ylabel('BARC LN Equity') set the labels for the x-axis and y-axis, respectively.\n",
    "#plt.title('Linear Regression Model') sets the title of the plot.\n",
    "#plt.legend() adds a legend to the plot, displaying the labels for the training data, test data, and trained model.\n",
    "#plt.show() displays the plot.\n",
    "#This plot allows you to visually assess how well the trained linear regression model fits the training and test data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X, y, label='Full', alpha=0.3, color='black')\n",
    "plt.scatter(X_train, y_train, label='Train', alpha=0.3, color='red')\n",
    "plt.scatter(X_test, y_test, label='Test', alpha=0.3, color='green')\n",
    "plt.scatter(X_test, y_test_pred, label='Predicted', alpha=0.3, color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('BAC LN Equity')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af864fb-82ca-406a-b55a-04c8d33f90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_test, y_test, label='Test Data', alpha=0.3)\n",
    "plt.scatter(X_test, model.predict(X_test), label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('BAC LN Equity')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12481885-4b41-4df0-b837-82dba35c3dd0",
   "metadata": {},
   "source": [
    "#### <u>Reviewing of the Original Dataset For BARC.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c93a6-72bd-43f8-9774-110bdc726db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line reads the data from the test_csv.csv file using the read_csv function from the pandas library and stores it in the data variable as a pandas DataFrame.\n",
    "\n",
    "stock_price = pd.read_csv(\"test_csv.csv\")\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "#This line converts the 'Dates' column in the DataFrame from string format to datetime format using the to_datetime function from pandas. \n",
    "#The format='%d/%m/%Y' argument specifies that the dates are in the 'DD/MM/YYYY' format.\n",
    "\n",
    "stock_price['Dates'] = pd.to_datetime(stock_price['Dates'], format='%d/%m/%Y')\n",
    "\n",
    "stock_price.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b38b0c-59a7-447d-bfe3-e8a824b3af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date column as the index\n",
    "#This line sets the 'Dates' column as the index of the DataFrame using the set_index method from pandas.\n",
    "\n",
    "stock_price = stock_price.set_index('Dates')\n",
    "stock_price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19e252-79c1-4ea7-885b-f462aba17f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datetime index to a numerical representation (number of days since a reference date)\n",
    "reference_date = stock_price.index.min()\n",
    "X = (stock_price.index - reference_date).days.values.reshape(-1, 1)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a18078-f82c-4eb3-81b1-119861f8f42d",
   "metadata": {},
   "source": [
    "#### <u>Splitting and Training</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073d33c-7491-4b8c-a925-2b3ee9f9b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "#This line assigns the 'BARC LN Equity' column from the DataFrame to the y variable, which represents the target variable (stock prices). \n",
    "#The values attribute is used to convert the pandas Series to a numpy array.\n",
    "\n",
    "y = stock_price['BARC LN Equity'].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "#This line splits the data into training and test sets using the train_test_split function from scikit-learn. \n",
    "#The X and y variables contain the feature matrix and target variable, respectively. \n",
    "#The test_size=0.2 argument specifies that 20% of the data should be used for testing, and the remaining 80% for training. \n",
    "#The random_state=42 argument ensures reproducibility by setting a fixed seed for the random number generator.\n",
    "#The function returns four variables: X_train, X_test, y_train, and y_test, which contain the feature matrices and target variables for the training and test sets, respectively.\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "tests_size = len(X)-train_size\n",
    "X_train, X_test = X[0:train_size],X[train_size:len(X)]\n",
    "y_train, y_test = y[0:train_size],y[train_size:len(X)]\n",
    "\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cdc51-cfe7-4fa3-a5d3-f635efa746f2",
   "metadata": {},
   "source": [
    "#### <u>Training The Model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cfbad3-ce48-44d2-833f-9d8e52f62c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the linear regression model\n",
    "#These lines create an instance of the LinearRegression model from scikit-learn and fit the model to the training data (X_train, y_train) using the fit method.\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the training, validation, and test sets\n",
    "#These lines use the fitted linear regression model to make predictions on the training and test sets using the predict method. \n",
    "#The predicted values for the training set are stored in y_train_pred, and the predicted values for the test set are stored in y_test_pred.\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127d02d-cbb8-407f-ad44-f73f74592f60",
   "metadata": {},
   "source": [
    "#### <u>PERFORMANCE EVALUATION BASED ON BARC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72673ecb-b5f7-4f79-8823-2ac1da05c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics on the training set\n",
    "#These lines calculate various evaluation metrics for assessing the performance of the linear regression model on both the training and test sets. The metrics calculated are:\n",
    "#r2_score: R-squared score, which measures how well the model fits the data (ranges from 0 to 1, with 1 being a perfect fit).\n",
    "#explained_variance_score: Explained variance score, which represents the proportion of variance in the target variable that is explained by the model.\n",
    "#mean_absolute_percentage_error: Mean Absolute Percentage Error (MAPE), which measures the average absolute percentage difference between the predicted and actual values.\n",
    "#mean_squared_error: Mean Squared Error (MSE), which measures the average squared difference between the predicted and actual values.\n",
    "#np.sqrt(mse): Root Mean Squared Error (RMSE), which is the square root of the MSE and provides a measure of the typical magnitude of the prediction errors.\n",
    "#mean_absolute_error: Mean Absolute Error (MAE), which measures the average absolute difference between the predicted and actual values.\n",
    "#Each metric is calculated separately for the training and test sets using the corresponding functions from the sklearn.metrics module.\n",
    "\n",
    "\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "explained_variance_train = explained_variance_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = mse_train ** 0.5\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics on the test set\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "explained_variance_test = explained_variance_score(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "#These lines print the calculated evaluation metrics for both the training and test sets. The f-strings are used to format the output with the metric names and values.\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_train:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_train:.2f}\")\n",
    "print(f\"MAPE: {mape_train:.2f}\")\n",
    "print(f\"MSE: {mse_train:.2f}\")\n",
    "print(f\"RMSE: {rmse_train:.2f}\")\n",
    "print(f\"MAE: {mae_train:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "\n",
    "print(f\"\\nR-squared: {r2_test:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_test:.2f}\")\n",
    "print(f\"MAPE: {mape_test:.2f}\")\n",
    "print(f\"MSE: {mse_test:.2f}\")\n",
    "print(f\"RMSE: {rmse_test:.2f}\")\n",
    "print(f\"MAE: {mae_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c434412a-9927-4a1f-a5a8-ef5f584106ae",
   "metadata": {},
   "source": [
    "#### **VISUALIZING OUR FORECAST FOR BARC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a802e-0f0f-4f36-94e4-9a4a3ab6f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test data with the trained model\n",
    "#These lines create a plot using Matplotlib to visualize the training and test data along with the trained linear regression model:\n",
    "\n",
    "#plt.figure(figsize=(12, 6)) creates a new figure with a specific size (12 inches wide and 6 inches tall).\n",
    "#plt.scatter(X_train, y_train, label='Training Data', alpha=0.3) plots the training data as scattered points, with a label and transparency level (alpha=0.3).\n",
    "#plt.scatter(X_test, y_test, label='Test Data', alpha=0.3) plots the test data as scattered points, with a label and transparency level (alpha=0.3).\n",
    "#plt.plot(X, model.predict(X), 'r', label='Trained Model') plots the trained linear regression model as a red line, using the predict method to obtain the predicted values for the entire feature matrix X.\n",
    "#plt.xlabel('Date') and plt.ylabel('BARC LN Equity') set the labels for the x-axis and y-axis, respectively.\n",
    "#plt.title('Linear Regression Model') sets the title of the plot.\n",
    "#plt.legend() adds a legend to the plot, displaying the labels for the training data, test data, and trained model.\n",
    "#plt.show() displays the plot.\n",
    "#This plot allows you to visually assess how well the trained linear regression model fits the training and test data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X, y, label='Full', alpha=0.3, color='black')\n",
    "plt.scatter(X_train, y_train, label='Train', alpha=0.3, color='red')\n",
    "plt.scatter(X_test, y_test, label='Test', alpha=0.3, color='green')\n",
    "plt.scatter(X_test, y_test_pred, label='Predicted', alpha=0.3, color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('BARC LN Equity')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48446c-13fc-4fb0-8eee-46621af949a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_test, y_test, label='Test Data', alpha=0.3)\n",
    "plt.scatter(X_test, model.predict(X_test), label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('BARC LN Equity')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ba3d5-d12f-4f2b-88e2-406bb52ee16e",
   "metadata": {},
   "source": [
    "#### <u>Reviewing of the Original Dataset For JPM</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b108a-e8be-4c37-9978-5fbe0b0ea2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line reads the data from the test_csv.csv file using the read_csv function from the pandas library and stores it in the data variable as a pandas DataFrame.\n",
    "\n",
    "stock_price = pd.read_csv(\"Stockprice_three.csv\")\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "#This line converts the 'Dates' column in the DataFrame from string format to datetime format using the to_datetime function from pandas. \n",
    "#The format='%d/%m/%Y' argument specifies that the dates are in the 'DD/MM/YYYY' format.\n",
    "\n",
    "stock_price['Dates'] = pd.to_datetime(stock_price['Dates'], format='%d/%m/%Y')\n",
    "\n",
    "stock_price.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1573741-efd1-43da-bf2b-ffda3eddc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date column as the index\n",
    "#This line sets the 'Dates' column as the index of the DataFrame using the set_index method from pandas.\n",
    "\n",
    "stock_price = stock_price.set_index('Dates')\n",
    "stock_price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f692fc1-44b8-4cbf-8bc1-624a0d64e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datetime index to a numerical representation (number of days since a reference date)\n",
    "reference_date = stock_price.index.min()\n",
    "X = (stock_price.index - reference_date).days.values.reshape(-1, 1)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b92cdc-e9ee-4caa-a145-97995bd1a499",
   "metadata": {},
   "source": [
    "#### <u>Splitting and Training</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97627af0-76c1-4e3c-87f4-6f42491c5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "#This line assigns the 'BARC LN Equity' column from the DataFrame to the y variable, which represents the target variable (stock prices). \n",
    "#The values attribute is used to convert the pandas Series to a numpy array.\n",
    "\n",
    "y = stock_price['JPM UN Equity'].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "#This line splits the data into training and test sets using the train_test_split function from scikit-learn. \n",
    "#The X and y variables contain the feature matrix and target variable, respectively. \n",
    "#The test_size=0.2 argument specifies that 20% of the data should be used for testing, and the remaining 80% for training. \n",
    "#The random_state=42 argument ensures reproducibility by setting a fixed seed for the random number generator.\n",
    "#The function returns four variables: X_train, X_test, y_train, and y_test, which contain the feature matrices and target variables for the training and test sets, respectively.\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "tests_size = len(X)-train_size\n",
    "X_train, X_test = X[0:train_size],X[train_size:len(X)]\n",
    "y_train, y_test = y[0:train_size],y[train_size:len(X)]\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464087dd-0199-4e05-a705-221994a54b7b",
   "metadata": {},
   "source": [
    "#### <u>Training The Model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5587fd-6514-4624-bde2-acf3839d3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the linear regression model\n",
    "#These lines create an instance of the LinearRegression model from scikit-learn and fit the model to the training data (X_train, y_train) using the fit method.\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training, validation, and test sets\n",
    "#These lines use the fitted linear regression model to make predictions on the training and test sets using the predict method. \n",
    "#The predicted values for the training set are stored in y_train_pred, and the predicted values for the test set are stored in y_test_pred.\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f9a1c-0137-42c8-9f12-63dbb1301f23",
   "metadata": {},
   "source": [
    "#### <u>PERFORMANCE EVALUATION BASED ON JPM</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11374a28-8d73-481e-b41a-6c39fa593170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics on the training set\n",
    "#These lines calculate various evaluation metrics for assessing the performance of the linear regression model on both the training and test sets. The metrics calculated are:\n",
    "#r2_score: R-squared score, which measures how well the model fits the data (ranges from 0 to 1, with 1 being a perfect fit).\n",
    "#explained_variance_score: Explained variance score, which represents the proportion of variance in the target variable that is explained by the model.\n",
    "#mean_absolute_percentage_error: Mean Absolute Percentage Error (MAPE), which measures the average absolute percentage difference between the predicted and actual values.\n",
    "#mean_squared_error: Mean Squared Error (MSE), which measures the average squared difference between the predicted and actual values.\n",
    "#np.sqrt(mse): Root Mean Squared Error (RMSE), which is the square root of the MSE and provides a measure of the typical magnitude of the prediction errors.\n",
    "#mean_absolute_error: Mean Absolute Error (MAE), which measures the average absolute difference between the predicted and actual values.\n",
    "#Each metric is calculated separately for the training and test sets using the corresponding functions from the sklearn.metrics module.\n",
    "\n",
    "\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "explained_variance_train = explained_variance_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = mse_train ** 0.5\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate evaluation metrics on the test set\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "explained_variance_test = explained_variance_score(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "#These lines print the calculated evaluation metrics for both the training and test sets. The f-strings are used to format the output with the metric names and values.\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_train:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_train:.2f}\")\n",
    "print(f\"MAPE: {mape_train:.2f}\")\n",
    "print(f\"MSE: {mse_train:.2f}\")\n",
    "print(f\"RMSE: {rmse_train:.2f}\")\n",
    "print(f\"MAE: {mae_train:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "\n",
    "print(f\"\\nR-squared: {r2_test:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_test:.2f}\")\n",
    "print(f\"MAPE: {mape_test:.2f}\")\n",
    "print(f\"MSE: {mse_test:.2f}\")\n",
    "print(f\"RMSE: {rmse_test:.2f}\")\n",
    "print(f\"MAE: {mae_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936aa8de-ad66-42d8-bef3-41ca3fd98ba0",
   "metadata": {},
   "source": [
    "#### **VISUALIZING OUR FORECAST FOR JPM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc66de-fe1b-43cd-9f5f-96958abee6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test data with the trained model\n",
    "#These lines create a plot using Matplotlib to visualize the training and test data along with the trained linear regression model:\n",
    "\n",
    "#plt.figure(figsize=(12, 6)) creates a new figure with a specific size (12 inches wide and 6 inches tall).\n",
    "#plt.scatter(X_train, y_train, label='Training Data', alpha=0.3) plots the training data as scattered points, with a label and transparency level (alpha=0.3).\n",
    "#plt.scatter(X_test, y_test, label='Test Data', alpha=0.3) plots the test data as scattered points, with a label and transparency level (alpha=0.3).\n",
    "#plt.plot(X, model.predict(X), 'r', label='Trained Model') plots the trained linear regression model as a red line, using the predict method to obtain the predicted values for the entire feature matrix X.\n",
    "#plt.xlabel('Date') and plt.ylabel('BARC LN Equity') set the labels for the x-axis and y-axis, respectively.\n",
    "#plt.title('Linear Regression Model') sets the title of the plot.\n",
    "#plt.legend() adds a legend to the plot, displaying the labels for the training data, test data, and trained model.\n",
    "#plt.show() displays the plot.\n",
    "#This plot allows you to visually assess how well the trained linear regression model fits the training and test data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X, y, label='Full', alpha=0.3, color='black')\n",
    "plt.scatter(X_train, y_train, label='Train', alpha=0.3, color='red')\n",
    "plt.scatter(X_test, y_test, label='Test', alpha=0.3, color='green')\n",
    "plt.scatter(X_test, y_test_pred, label='Predicted', alpha=0.3, color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('JPM UN Equity')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23ab98-587d-45ff-a3d3-30046c702ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_test, y_test, label='Test Data', alpha=0.3)\n",
    "plt.scatter(X_test, model.predict(X_test), label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('JPM UN Equity')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912876a5-f6b2-456a-aa3e-204dc799dd9d",
   "metadata": {},
   "source": [
    "#### <u>Reviewing of the Original Dataset For HSBA</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7efec-961e-444f-9a0a-2ce6294a964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line reads the data from the test_csv.csv file using the read_csv function from the pandas library and stores it in the data variable as a pandas DataFrame.\n",
    "\n",
    "stock_price = pd.read_csv(\"Stockprice_two.csv\")\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "#This line converts the 'Dates' column in the DataFrame from string format to datetime format using the to_datetime function from pandas. \n",
    "#The format='%d/%m/%Y' argument specifies that the dates are in the 'DD/MM/YYYY' format.\n",
    "\n",
    "stock_price['Dates'] = pd.to_datetime(stock_price['Dates'], format='%d/%m/%Y')\n",
    "\n",
    "stock_price.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973f183-9301-4e79-95f2-00329457215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the date column as the index\n",
    "#This line sets the 'Dates' column as the index of the DataFrame using the set_index method from pandas.\n",
    "\n",
    "stock_price = stock_price.set_index('Dates')\n",
    "stock_price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d4990-8819-43ff-8620-38d4fb001969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datetime index to a numerical representation (number of days since a reference date)\n",
    "reference_date = stock_price.index.min()\n",
    "X = (stock_price.index - reference_date).days.values.reshape(-1, 1)\n",
    "len(X)\n",
    "y = stock_price['HSBC Equity'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071ad08-6464-4c84-89d3-c3fde1274b7a",
   "metadata": {},
   "source": [
    "#### <u>Splitting and Training</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52631f36-26a3-4614-ad0f-d5a329da8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "#This line assigns the 'BARC LN Equity' column from the DataFrame to the y variable, which represents the target variable (stock prices). \n",
    "#The values attribute is used to convert the pandas Series to a numpy array.\n",
    "\n",
    "y = stock_price['HSBC Equity'].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "#This line splits the data into training and test sets using the train_test_split function from scikit-learn. \n",
    "#The X and y variables contain the feature matrix and target variable, respectively. \n",
    "#The test_size=0.2 argument specifies that 20% of the data should be used for testing, and the remaining 80% for training. \n",
    "#The random_state=42 argument ensures reproducibility by setting a fixed seed for the random number generator.\n",
    "#The function returns four variables: X_train, X_test, y_train, and y_test, which contain the feature matrices and target variables for the training and test sets, respectively.\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "tests_size = len(X)-train_size\n",
    "X_train, X_test = X[0:train_size],X[train_size:len(X)]\n",
    "y_train, y_test = y[0:train_size],y[train_size:len(X)]\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c28ed-b809-4792-ae59-00fc3d672408",
   "metadata": {},
   "source": [
    "#### <u>Training The Model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d2928-7982-4591-856f-d406832d76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "#These lines create an instance of the LinearRegression model from scikit-learn and fit the model to the training data (X_train, y_train) using the fit method.\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#X_train\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "#These lines create an instance of the LinearRegression model from scikit-learn and fit the model to the training data (X_train, y_train) using the fit method.\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training, validation, and test sets\n",
    "#These lines use the fitted linear regression model to make predictions on the training and test sets using the predict method. \n",
    "#The predicted values for the training set are stored in y_train_pred, and the predicted values for the test set are stored in y_test_pred.\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db2d55-dbe9-44e9-8463-497afacca378",
   "metadata": {},
   "source": [
    "#### <u>PERFORMANCE EVALUATION BASED ON HSBA</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f1f0e-fb47-4b32-82a0-62687486c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics on the training set\n",
    "#These lines calculate various evaluation metrics for assessing the performance of the linear regression model on both the training and test sets. The metrics calculated are:\n",
    "#r2_score: R-squared score, which measures how well the model fits the data (ranges from 0 to 1, with 1 being a perfect fit).\n",
    "#explained_variance_score: Explained variance score, which represents the proportion of variance in the target variable that is explained by the model.\n",
    "#mean_absolute_percentage_error: Mean Absolute Percentage Error (MAPE), which measures the average absolute percentage difference between the predicted and actual values.\n",
    "#mean_squared_error: Mean Squared Error (MSE), which measures the average squared difference between the predicted and actual values.\n",
    "#np.sqrt(mse): Root Mean Squared Error (RMSE), which is the square root of the MSE and provides a measure of the typical magnitude of the prediction errors.\n",
    "#mean_absolute_error: Mean Absolute Error (MAE), which measures the average absolute difference between the predicted and actual values.\n",
    "#Each metric is calculated separately for the training and test sets using the corresponding functions from the sklearn.metrics module.\n",
    "\n",
    "\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "explained_variance_train = explained_variance_score(y_train, y_train_pred)\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "rmse_train = mse_train ** 0.5\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate evaluation metrics on the test set\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "explained_variance_test = explained_variance_score(y_test, y_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mse_test ** 0.5\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "#These lines print the calculated evaluation metrics for both the training and test sets. The f-strings are used to format the output with the metric names and values.\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(f\"\\nR-squared: {r2_train:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_train:.2f}\")\n",
    "print(f\"MAPE: {mape_train:.2f}\")\n",
    "print(f\"MSE: {mse_train:.2f}\")\n",
    "print(f\"RMSE: {rmse_train:.2f}\")\n",
    "print(f\"MAE: {mae_train:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "\n",
    "print(f\"\\nR-squared: {r2_test:.2f}\")\n",
    "print(f\"Explained Variation: {explained_variance_test:.2f}\")\n",
    "print(f\"MAPE: {mape_test:.2f}\")\n",
    "print(f\"MSE: {mse_test:.2f}\")\n",
    "print(f\"RMSE: {rmse_test:.2f}\")\n",
    "print(f\"MAE: {mae_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43e3b5-b144-4004-9945-3c4aff2b1725",
   "metadata": {},
   "source": [
    "#### **VISUALIZING OUR FORECAST FOR HSBA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8141f1-ae20-41cd-8ea2-3ecfd25cc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test data with the trained model\n",
    "#These lines create a plot using Matplotlib to visualize the training and test data along with the trained linear regression model:\n",
    "\n",
    "#plt.figure(figsize=(12, 6)) creates a new figure with a specific size (12 inches wide and 6 inches tall).\n",
    "#plt.scatter(X_train, y_train, label='Training Data', alpha=0.3) plots the training data as scattered points, with a label and transparency level (alpha=0.3).\n",
    "#plt.scatter(X_test, y_test, label='Test Data', alpha=0.3) plots the test data as scattered points, with a label and transparency level (alpha=0.3).\n",
    "#plt.plot(X, model.predict(X), 'r', label='Trained Model') plots the trained linear regression model as a red line, using the predict method to obtain the predicted values for the entire feature matrix X.\n",
    "#plt.xlabel('Date') and plt.ylabel('BARC LN Equity') set the labels for the x-axis and y-axis, respectively.\n",
    "#plt.title('Linear Regression Model') sets the title of the plot.\n",
    "#plt.legend() adds a legend to the plot, displaying the labels for the training data, test data, and trained model.\n",
    "#plt.show() displays the plot.\n",
    "#This plot allows you to visually assess how well the trained linear regression model fits the training and test data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X, y, label='Full', alpha=0.3, color='black')\n",
    "plt.scatter(X_train, y_train, label='Train', alpha=0.3, color='red')\n",
    "plt.scatter(X_test, y_test, label='Test', alpha=0.3, color='green')\n",
    "plt.scatter(X_test, y_test_pred, label='Predicted', alpha=0.3, color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('HSBA Equity')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5facf6d3-e7d1-4713-a4c7-51f6d5f8d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_test, y_test, label='Test Data', alpha=0.3)\n",
    "plt.scatter(X_test, model.predict(X_test), label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('HSBA Equity')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df275dfa-64cb-4e0e-8e48-fb75298edd07",
   "metadata": {},
   "source": [
    "# <center>**STOCK PRICE PREDICTION USING ARIMA MODEL**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bfe34-2cf7-4a26-bff6-f5bf12aa7071",
   "metadata": {},
   "source": [
    "#### <u>Reviewing of the Original Dataset For HSBA</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764a878-54fc-4def-bfbe-8afff86040fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('stockprice_two.csv')\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b4063-7942-4d58-aab8-9e63b51efd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1 = pd.read_csv('stockprice_two.csv')\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874761ae-b81a-467b-b24e-1d8af7542491",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1\n",
    "df_2['Dates'] = pd.to_datetime(df_1['Dates'], format='%d/%m/%Y')\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "\n",
    "# Set 'Dates' as the index of the DataFrame\n",
    "df_2 = df_2.set_index('Dates')\n",
    "if not isinstance(df_2.index, pd.DatetimeIndex):\n",
    "    # If not, set the frequency to 'D'\n",
    "    df_2.index = pd.DatetimeIndex(df_2.index, freq='D')\n",
    "\n",
    "#Check for null \n",
    "\n",
    "df_2.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71350c6f-ffb0-489b-b4b4-19004ff92310",
   "metadata": {},
   "source": [
    "#### **VISUALIZING OUR FORECAST FOR HSBA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cb7a1-699b-43e5-8842-cb25d982f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot trendline with date and stockprice\n",
    "\n",
    "year = df_2.index\n",
    "stock_price = df_2['HSBC Equity']\n",
    "plt.figure(figsize=(25, 8))\n",
    "\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('stock_price prediction')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('stock_price')\n",
    "plt.xticks(rotation=360, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f6ab4-25ef-4b14-b742-af9ac018d47c",
   "metadata": {},
   "source": [
    "# Augmented Dickey Fuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7cd0c1-fa27-4706-9fa2-ebef53601cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ADF test\n",
    "result = adfuller(stock_price)\n",
    "\n",
    "# Extract and print the results\n",
    "adf_statistic = result[0]\n",
    "p_value = result[1]\n",
    "critical_values = result[4]\n",
    "\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "print('Critical Values:')\n",
    "for key, value in critical_values.items():\n",
    "    print(f'   {key}: {value}')\n",
    "\n",
    "# Check for stationarity based on the p-value\n",
    "if p_value <= 0.05:\n",
    "    print(\"Reject the null hypothesis. The data is likely stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. The data may not be stationary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b50a05-10ad-477c-852b-62299cda77e7",
   "metadata": {},
   "source": [
    "# Autocorrelation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cbd0ba-736a-4fef-8073-756d1b58c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot ACF\n",
    "plot_acf(stock_price, lags=30, title='Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "plot_pacf(stock_price, lags=30, title='Partial Autocorrelation Function (PACF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c0697-de2e-4623-91b9-41bc47b376b5",
   "metadata": {},
   "source": [
    "# Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438cf3a-1a5c-442c-9dad-963d530cd8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(data, window, 'same') \n",
    "\n",
    "#data = df_2['BARC LN Equity'] # example \n",
    "window_size = 30 # size of moving window\n",
    "\n",
    "ma = moving_average(stock_price, window_size)\n",
    "print(ma)\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.plot(year, ma)\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('Stock Price Moving Average')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9b7d4-b6ce-49db-b46e-cd21cf48c0f8",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5fbb5-75cf-4138-b8f1-b9f489e1f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decomposition\n",
    "decomposition = seasonal_decompose(df_2['HSBC Equity'], model='additive', period=365)\n",
    "\n",
    "\n",
    "# Plot the decomposition components\n",
    "fig, ax = plt.subplots(4, 1, figsize=(10, 8))\n",
    "\n",
    "ax[0].plot(df_2.index, decomposition.observed)\n",
    "ax[0].set_title('Original Series')\n",
    "\n",
    "ax[1].plot(df_2.index, decomposition.trend)\n",
    "ax[1].set_title('Trend')\n",
    "\n",
    "ax[2].plot(df_2.index, decomposition.seasonal)\n",
    "ax[2].set_title('Seasonality')\n",
    "\n",
    "ax[3].plot(df_2.index, decomposition.resid)\n",
    "ax[3].set_title('Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9c80b-8375-4e48-97ff-89d802b232d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_data, test_data = train_test_split(df_2, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Fit Auto ARIMA model\n",
    "model = auto_arima(train_data['HSBC Equity'], seasonal=True, m=12, suppress_warnings=True)\n",
    "\n",
    "# Make predictions\n",
    "forecast = model.predict(n_periods=len(test_data))\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(test_data['HSBC Equity'], forecast)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd08b2-307b-4dde-bd0d-6392b9461fb9",
   "metadata": {},
   "source": [
    "# Auto Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0009e8b-e214-4a2f-8ba0-79c957e72d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Use auto_arima to find the best model\n",
    "model = auto_arima(df_2['HSBC Equity'], seasonal=True, m=12, suppress_warnings=True)\n",
    "\n",
    "# Get the selected order\n",
    "selected_order = model.order\n",
    "print(\"Selected ARIMA Order:\", selected_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef95fa6-19db-4431-94f4-f8c1f18c0b39",
   "metadata": {},
   "source": [
    "# AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d4aa3-0b63-4033-940d-89d26110f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['HSBC Equity'], label='Original Data', marker='o')\n",
    "plt.title('Original HSBC Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('HSBC Equity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the 'StockPrice' column\n",
    "df_2['DifferencedHSBC Equity'] = df_2['HSBC Equity'].diff()\n",
    "\n",
    "# Display the differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['DifferencedHSBC Equity'], label='Differenced Data', marker='o', color='orange')\n",
    "plt.title('Differenced HSBC Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Differenced HSBC Equity')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13db28-91b1-4cc4-921b-11873bb1dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['DifferencedHSBC Equity'] #= #df['BARC LN Equity'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73cc763-27c0-4206-8aab-c87dd70f6232",
   "metadata": {},
   "source": [
    "# second order differencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1279d-e661-47ba-ae6e-bb620dede238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the file path with the location of your dataset\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['HSBC Equity'], label='Original Data', marker='o')\n",
    "plt.title('Original HSBC Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('HSBC Equity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the 'StockPrice' column\n",
    "#df['DifferencedHSBC Equity'] = df['BARC LN Equity'].diff()\n",
    "\n",
    "# Display the first differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['DifferencedHSBC Equity'], label='1st Differenced Data', marker='o', color='orange')\n",
    "plt.title('1st Differenced HSBC Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('1st Differenced HSBC Equity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the already differenced data\n",
    "df_2['SecondDifferencedHSBC Equity'] = df_2['DifferencedHSBC Equity'].diff()\n",
    "\n",
    "# Display the second differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['SecondDifferencedHSBC Equity'], label='2nd Differenced Data', marker='o', color='green')\n",
    "plt.title('2nd DifferencedHSBC Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('2nd Differenced HSBC Equity')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91f61e-fec0-4a20-aef1-1c4839fcd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA (AutoRegressive Integrated Moving Average) model\n",
    "\n",
    "# Plot the original time series\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(df_2.index, df_2['HSBC Equity'], label='Original Data')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Stock Price')\n",
    "\n",
    "# Fit ARIMA model\n",
    "order = (0, 1, 0)  # Replace p, d, q with appropriate values\n",
    "model = ARIMA(df_2['HSBC Equity'], order=order)\n",
    "result = model.fit()\n",
    "\n",
    "# Get predictions\n",
    "predictions = result.predict(start=df_2.index.min(), end=df_2.index.max(), dynamic=False)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.plot(df_2.index, predictions, color='red', label='ARIMA Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b60465-a050-46ac-be65-37d96d5f443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform a grid search for ARIMA parameters using AIC\n",
    "# Define the range of values for p, d, and q\n",
    "p_values = [0, 1, 2]\n",
    "d_values = [0, 1]\n",
    "q_values = [0, 1, 2]\n",
    "\n",
    "# Initialize variables for optimal values and minimum AIC\n",
    "best_aic = float(\"inf\")\n",
    "best_order = None\n",
    "\n",
    "# Perform grid search\n",
    "for p, d, q in itertools.product(p_values, d_values, q_values):\n",
    "    order = (p, d, q)\n",
    "    try:\n",
    "        model = ARIMA(df_2['HSBC Equity'], order=order)\n",
    "        result = model.fit()\n",
    "        aic = result.aic\n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            best_order = order\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"Best AIC: {best_aic}\")\n",
    "print(f\"Best Order (p, d, q): {best_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514ced5-5a26-425e-8328-5c8f5f4e64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_date = df_2.index.min()\n",
    "X = (df_2.index - reference_date).days.values.reshape(-1, 1)\n",
    "y = df_2['HSBC Equity'].values\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "tests_size = len(X)-train_size\n",
    "X_train, X_test = X[0:train_size],X[train_size:len(X)]\n",
    "y_train, y_test = y[0:train_size],y[train_size:len(X)]\n",
    "\n",
    "# Use auto_arima to find the best ARIMA model\n",
    "model = auto_arima(df_2['HSBC Equity'], seasonal=True, m=12, suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Get the selected order from auto_arima\n",
    "order = model.get_params()['order']\n",
    "\n",
    "# Train ARIMA model with the selected order\n",
    "arima_model = ARIMA(df_2['HSBC Equity'], order=order)\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "# Forecast using the trained ARIMA model\n",
    "forecast = arima_fit.forecast(steps=len(X_test))\n",
    "\n",
    "# Evaluate the model using AIC\n",
    "aic = arima_fit.aic\n",
    "\n",
    "y_train_pred = model.predict(n_periods=len(X_train))\n",
    "y_test_pred = model.predict(n_periods=len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f34c56-7729-4f1f-9a6a-df2afa699d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Selected ARIMA Order: {order}\")\n",
    "print(f\"AIC: {aic}\")\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('stock_price prediction')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('stock_price')\n",
    "plt.xticks(rotation=360, ha='right')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train, y_train, label='Train', alpha=0.3, color='red')\n",
    "plt.scatter(X_test, y_test, label='Test', alpha=0.3, color='green')\n",
    "plt.scatter(X_test, y_test_pred, label='Predicted', alpha=0.3, color='orange')\n",
    "plt.title('Stock Price Prediction with Auto ARIMA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51591b4-1a57-4c33-9575-5fd11f28195c",
   "metadata": {},
   "source": [
    "#### <u>Reviewing of the Original Dataset For BARC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c7a51-85ad-4b93-8172-8fbe962c09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('test_csv.csv')\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13b5fe-001a-4710-b3f0-bccaa8304671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1 = pd.read_csv('test_csv.csv')\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e0f8c0-4252-4577-9bc0-cbc9fd83b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1\n",
    "df_2['Dates'] = pd.to_datetime(df_1['Dates'], format='%d/%m/%Y')\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "\n",
    "# Set 'Dates' as the index of the DataFrame\n",
    "df_2 = df_2.set_index('Dates')\n",
    "if not isinstance(df_2.index, pd.DatetimeIndex):\n",
    "    # If not, set the frequency to 'D'\n",
    "    df_2.index = pd.DatetimeIndex(df_2.index, freq='D')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7694f8-1b46-4431-83fe-b87f148f2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null \n",
    "\n",
    "df_2.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3539a-5830-422d-8aaf-0a82153a01b4",
   "metadata": {},
   "source": [
    "# Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466be9ed-f0ab-444f-8854-b6b1982b432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot trendline with date and stockprice\n",
    "\n",
    "year = df_2.index\n",
    "stock_price = df_2['BARC LN Equity']\n",
    "plt.figure(figsize=(25, 8))\n",
    "\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('stock_price prediction')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('stock_price')\n",
    "plt.xticks(rotation=360, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8412f8-d293-4137-adc6-49bb0d921a7a",
   "metadata": {},
   "source": [
    "# Augmented Dickey Fuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552f12c-965e-407d-84cf-bce27bc9892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ADF test\n",
    "result = adfuller(stock_price)\n",
    "\n",
    "# Extract and print the results\n",
    "adf_statistic = result[0]\n",
    "p_value = result[1]\n",
    "critical_values = result[4]\n",
    "\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "print('Critical Values:')\n",
    "for key, value in critical_values.items():\n",
    "    print(f'   {key}: {value}')\n",
    "\n",
    "# Check for stationarity based on the p-value\n",
    "if p_value <= 0.05:\n",
    "    print(\"Reject the null hypothesis. The data is likely stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. The data may not be stationary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3564d45-9262-48e2-a71d-92960d244cf3",
   "metadata": {},
   "source": [
    "# Autocorrelation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d4472-a730-40c4-a7ee-0a42477caad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot ACF\n",
    "plot_acf(stock_price, lags=30, title='Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "plot_pacf(stock_price, lags=30, title='Partial Autocorrelation Function (PACF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fc561-a5e3-41a1-9493-8d2992012814",
   "metadata": {},
   "source": [
    "# Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba34a7a-19e7-4528-9d9e-d59865b0fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(data, window, 'same') \n",
    "\n",
    "#data = df_2['BARC LN Equity'] # example \n",
    "window_size = 30 # size of moving window\n",
    "\n",
    "ma = moving_average(stock_price, window_size)\n",
    "print(ma)\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.plot(year, ma)\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('Stock Price Moving Average')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6c7a4-0dbf-44c6-874a-0b7e9ccaf8f0",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3d26f-bd56-4779-b4fb-3381f8872449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decomposition\n",
    "decomposition = seasonal_decompose(df_2['BARC LN Equity'], model='additive', period=365)\n",
    "\n",
    "\n",
    "# Plot the decomposition components\n",
    "fig, ax = plt.subplots(4, 1, figsize=(10, 8))\n",
    "\n",
    "ax[0].plot(df_2.index, decomposition.observed)\n",
    "ax[0].set_title('Original Series')\n",
    "\n",
    "ax[1].plot(df_2.index, decomposition.trend)\n",
    "ax[1].set_title('Trend')\n",
    "\n",
    "ax[2].plot(df_2.index, decomposition.seasonal)\n",
    "ax[2].set_title('Seasonality')\n",
    "\n",
    "ax[3].plot(df_2.index, decomposition.resid)\n",
    "ax[3].set_title('Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d2e91-90b5-49e3-a3c9-96537dd74028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_data, test_data = train_test_split(df_2, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Fit Auto ARIMA model\n",
    "model = auto_arima(train_data['BARC LN Equity'], seasonal=True, m=12, suppress_warnings=True)\n",
    "\n",
    "# Make predictions\n",
    "forecast = model.predict(n_periods=len(test_data))\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(test_data['BARC LN Equity'], forecast)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea25f02-d223-4a65-9dc5-f866f180ccbd",
   "metadata": {},
   "source": [
    "# Auto Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296da71-8d52-4380-abdf-28ba9a1dac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Use auto_arima to find the best model\n",
    "model = auto_arima(df_2['BARC LN Equity'], seasonal=True, m=12, suppress_warnings=True)\n",
    "\n",
    "# Get the selected order\n",
    "selected_order = model.order\n",
    "print(\"Selected ARIMA Order:\", selected_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fff17-b61a-4f88-8841-9958548b4570",
   "metadata": {},
   "source": [
    "# AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0e27e-097a-49c2-8974-152c24af6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['BARC LN Equity'], label='Original Data', marker='o')\n",
    "plt.title('Original BARC LN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('BARC LN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the 'StockPrice' column\n",
    "df_2['DifferencedBARC LN Equity'] = df_2['BARC LN Equity'].diff()\n",
    "\n",
    "# Display the differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['DifferencedBARC LN Equity'], label='Differenced Data', marker='o', color='orange')\n",
    "plt.title('Differenced BARC LN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Differenced BARC LN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86d33a-7afd-4848-8c41-5ef53a00404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['DifferencedBARC LN Equity'] #= #df['BARC LN Equity'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c1942-7278-48d7-adee-1bfa16a2e554",
   "metadata": {},
   "source": [
    "# second order differencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eed98a-36e4-4662-aaf0-a094870b46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the file path with the location of your dataset\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['BARC LN Equity'], label='Original Data', marker='o')\n",
    "plt.title('Original BARC LN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('BARC LN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the 'StockPrice' column\n",
    "\n",
    "# Display the first differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['DifferencedBARC LN Equity'], label='1st Differenced Data', marker='o', color='orange')\n",
    "plt.title('1st Differenced BARC LN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('1st Differenced BARC LN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the already differenced data\n",
    "df_2['SecondDifferencedBARC LN Equity'] = df_2['DifferencedBARC LN Equity'].diff()\n",
    "\n",
    "# Display the second differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['SecondDifferencedBARC LN Equity'], label='2nd Differenced Data', marker='o', color='green')\n",
    "plt.title('2nd DifferencedBARC LN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('2nd Differenced BARC LN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c972b01-788f-47eb-8f1a-7a74cb891f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA (AutoRegressive Integrated Moving Average) model\n",
    "\n",
    "# Plot the original time series\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(df_2.index, df_2['BARC LN Equity'], label='Original Data')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Stock Price')\n",
    "\n",
    "# Fit ARIMA model\n",
    "order = (0, 1, 0)  # Replace p, d, q with appropriate values\n",
    "model = ARIMA(df_2['BARC LN Equity'], order=order)\n",
    "result = model.fit()\n",
    "\n",
    "# Get predictions\n",
    "predictions = result.predict(start=df_2.index.min(), end=df_2.index.max(), dynamic=False)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.plot(df_2.index, predictions, color='red', label='ARIMA Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac47ef-d10f-459c-bbc6-9d5c7775a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform a grid search for ARIMA parameters using AIC\n",
    "# Define the range of values for p, d, and q\n",
    "p_values = [0, 1, 2]\n",
    "d_values = [0, 1]\n",
    "q_values = [0, 1, 2]\n",
    "\n",
    "# Initialize variables for optimal values and minimum AIC\n",
    "best_aic = float(\"inf\")\n",
    "best_order = None\n",
    "\n",
    "# Perform grid search\n",
    "for p, d, q in itertools.product(p_values, d_values, q_values):\n",
    "    order = (p, d, q)\n",
    "    try:\n",
    "        model = ARIMA(df_2['HSBC Equity'], order=order)\n",
    "        result = model.fit()\n",
    "        aic = result.aic\n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            best_order = order\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"Best AIC: {best_aic}\")\n",
    "print(f\"Best Order (p, d, q): {best_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9122d-6c43-49ff-9f86-ec250f21bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_date = df_2.index.min()\n",
    "X = (df_2.index - reference_date).days.values.reshape(-1, 1)\n",
    "y = df_2['BARC LN Equity'].values\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "tests_size = len(X)-train_size\n",
    "X_train, X_test = X[0:train_size],X[train_size:len(X)]\n",
    "y_train, y_test = y[0:train_size],y[train_size:len(X)]\n",
    "\n",
    "# Use auto_arima to find the best ARIMA model\n",
    "model = auto_arima(df_2['BARC LN Equity'], seasonal=True, m=12, suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Get the selected order from auto_arima\n",
    "order = model.get_params()['order']\n",
    "\n",
    "# Train ARIMA model with the selected order\n",
    "arima_model = ARIMA(df_2['BARC LN Equity'], order=order)\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "# Forecast using the trained ARIMA model\n",
    "forecast = arima_fit.forecast(steps=len(X_test))\n",
    "\n",
    "# Evaluate the model using AIC\n",
    "aic = arima_fit.aic\n",
    "\n",
    "y_train_pred = model.predict(n_periods=len(X_train))\n",
    "y_test_pred = model.predict(n_periods=len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23698e36-180a-420f-9afb-607b14613100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Selected ARIMA Order: {order}\")\n",
    "print(f\"AIC: {aic}\")\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('stock_price prediction')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('stock_price')\n",
    "plt.xticks(rotation=360, ha='right')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train, y_train, label='Train', alpha=0.3, color='red')\n",
    "plt.scatter(X_test, y_test, label='Test', alpha=0.3, color='green')\n",
    "plt.scatter(X_test, y_test_pred, label='Predicted', alpha=0.3, color='orange')\n",
    "plt.title('Stock Price Prediction with Auto ARIMA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7acef-7aec-4395-baee-7a256fe887a2",
   "metadata": {},
   "source": [
    "#### <u>Reviewing of the Original Dataset For JPM</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f12e0-b14d-48a5-8b54-abbc90f3665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('stockprice_three.csv')\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0b8de-5b00-4bb2-a554-e53d87136d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('stockprice_three.csv')\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0705548-6a4c-4f0e-a4c5-7de11844f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1\n",
    "df_2['Dates'] = pd.to_datetime(df_1['Dates'], format='%d/%m/%Y')\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "\n",
    "# Set 'Dates' as the index of the DataFrame\n",
    "df_2 = df_2.set_index('Dates')\n",
    "if not isinstance(df_2.index, pd.DatetimeIndex):\n",
    "    # If not, set the frequency to 'D'\n",
    "    df_2.index = pd.DatetimeIndex(df_2.index, freq='D')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b8806-76cd-48a0-b48b-7be20d9d2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null \n",
    "\n",
    "df_2.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151a871-7823-41f9-b265-0fc2bef467c7",
   "metadata": {},
   "source": [
    "# Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a338cd-687d-40d4-ae29-12c7e50e6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot trendline with date and stockprice\n",
    "\n",
    "year = df_2.index\n",
    "stock_price = df_2['JPM UN Equity']\n",
    "plt.figure(figsize=(25, 8))\n",
    "\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('stock_price prediction')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('stock_price')\n",
    "plt.xticks(rotation=360, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ed4ba-7d25-4fd5-830c-61cc985a2878",
   "metadata": {},
   "source": [
    "# Augmented Dickey Fuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af8004-2688-4246-b93d-efd43e57724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ADF test\n",
    "result = adfuller(stock_price)\n",
    "\n",
    "# Extract and print the results\n",
    "adf_statistic = result[0]\n",
    "p_value = result[1]\n",
    "critical_values = result[4]\n",
    "\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "print('Critical Values:')\n",
    "for key, value in critical_values.items():\n",
    "    print(f'   {key}: {value}')\n",
    "\n",
    "# Check for stationarity based on the p-value\n",
    "if p_value <= 0.05:\n",
    "    print(\"Reject the null hypothesis. The data is likely stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. The data may not be stationary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebecdb71-893d-47b0-8d86-a02a7794619b",
   "metadata": {},
   "source": [
    "# Autocorrelation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd9d0e-05c3-4b75-8dbf-d40ac5ec1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot ACF\n",
    "plot_acf(stock_price, lags=30, title='Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "plot_pacf(stock_price, lags=30, title='Partial Autocorrelation Function (PACF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9d3bc-612c-4632-953c-f516b8be6d61",
   "metadata": {},
   "source": [
    "# Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d0438-20b4-43d7-a212-fa763a66dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(data, window, 'same') \n",
    "\n",
    "window_size = 30 # size of moving window\n",
    "\n",
    "ma = moving_average(stock_price, window_size)\n",
    "print(ma)\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.plot(year, ma)\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('Stock Price Moving Average')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32016a-3c09-4aed-9e0b-68cc49e5dbac",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15442019-68c2-4a22-a02d-cdf74715b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decomposition\n",
    "decomposition = seasonal_decompose(df_2['JPM UN Equity'], model='additive', period=365)\n",
    "\n",
    "\n",
    "# Plot the decomposition components\n",
    "fig, ax = plt.subplots(4, 1, figsize=(10, 8))\n",
    "\n",
    "ax[0].plot(df_2.index, decomposition.observed)\n",
    "ax[0].set_title('Original Series')\n",
    "\n",
    "ax[1].plot(df_2.index, decomposition.trend)\n",
    "ax[1].set_title('Trend')\n",
    "\n",
    "ax[2].plot(df_2.index, decomposition.seasonal)\n",
    "ax[2].set_title('Seasonality')\n",
    "\n",
    "ax[3].plot(df_2.index, decomposition.resid)\n",
    "ax[3].set_title('Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afaf28-a875-4b76-b1a8-278e4cb14e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_data, test_data = train_test_split(df_2, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Fit Auto ARIMA model\n",
    "model = auto_arima(train_data['JPM UN Equity'], seasonal=True, m=12, suppress_warnings=True)\n",
    "\n",
    "# Make predictions\n",
    "forecast = model.predict(n_periods=len(test_data))\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(test_data['JPM UN Equity'], forecast)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0e217-91be-4ad8-81e4-80b2123fd99f",
   "metadata": {},
   "source": [
    "# Auto Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627b45d-0b10-45da-aa79-f14e52038af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Use auto_arima to find the best model\n",
    "model = auto_arima(df_2['JPM UN Equity'], seasonal=True, m=12, suppress_warnings=True)\n",
    "\n",
    "# Get the selected order\n",
    "selected_order = model.order\n",
    "print(\"Selected ARIMA Order:\", selected_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6783c7-c39a-4936-9bcf-b8dc47645fd1",
   "metadata": {},
   "source": [
    "# AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ff0bb-d9d6-4381-abb2-23a88c4a697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['JPM UN Equity'], label='Original Data', marker='o')\n",
    "plt.title('Original JPM UN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('JPM UN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the 'StockPrice' column\n",
    "df_2['DifferencedJPM UN Equity'] = df_2['JPM UN Equity'].diff()\n",
    "\n",
    "# Display the differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['DifferencedJPM UN Equity'], label='Differenced Data', marker='o', color='orange')\n",
    "plt.title('Differenced JPM UN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Differenced JPM UN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01d296-6415-4324-90b5-ba9fbdadbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['DifferencedJPM UN Equity'] #= #df['BARC LN Equity'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59caf1ca-f639-46e2-a1ee-f8d9c3cf4972",
   "metadata": {},
   "source": [
    "# second order differencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922804e9-7e9c-45ed-b675-3249322ea0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the file path with the location of your dataset\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['JPM UN Equity'], label='Original Data', marker='o')\n",
    "plt.title('Original JPM UN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('JPM UN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the 'StockPrice' column\n",
    "\n",
    "# Display the first differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['DifferencedJPM UN Equity'], label='1st Differenced Data', marker='o', color='orange')\n",
    "plt.title('1st Differenced JPM UN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('1st Differenced JPM UN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the already differenced data\n",
    "df_2['SecondDifferencedJPM UN Equity'] = df_2['DifferencedJPM UN Equity'].diff()\n",
    "\n",
    "# Display the second differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['SecondDifferencedJPM UN Equity'], label='2nd Differenced Data', marker='o', color='green')\n",
    "plt.title('2nd DifferencedJPM UN Equity')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('2nd Differenced JPM UN Equity')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696e5a6-fc93-4d7e-af4e-2bc66ba5b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA (AutoRegressive Integrated Moving Average) model\n",
    "\n",
    "# Plot the original time series\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(df_2.index, df_2['JPM UN Equity'], label='Original Data')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Stock Price')\n",
    "\n",
    "# Fit ARIMA model\n",
    "order = (0, 1, 0)  # Replace p, d, q with appropriate values\n",
    "model = ARIMA(df_2['JPM UN Equity'], order=order)\n",
    "result = model.fit()\n",
    "\n",
    "# Get predictions\n",
    "predictions = result.predict(start=df_2.index.min(), end=df_2.index.max(), dynamic=False)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.plot(df_2.index, predictions, color='red', label='ARIMA Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd18e8-2463-40db-880f-308a4e9c13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform a grid search for ARIMA parameters using AIC\n",
    "# Define the range of values for p, d, and q\n",
    "p_values = [0, 1, 2]\n",
    "d_values = [0, 1]\n",
    "q_values = [0, 1, 2]\n",
    "\n",
    "# Initialize variables for optimal values and minimum AIC\n",
    "best_aic = float(\"inf\")\n",
    "best_order = None\n",
    "\n",
    "# Perform grid search\n",
    "for p, d, q in itertools.product(p_values, d_values, q_values):\n",
    "    order = (p, d, q)\n",
    "    try:\n",
    "        model = ARIMA(df_2['JPM UN Equity'], order=order)\n",
    "        result = model.fit()\n",
    "        aic = result.aic\n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            best_order = order\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"Best AIC: {best_aic}\")\n",
    "print(f\"Best Order (p, d, q): {best_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2d65d-8302-464a-b991-d3795d16f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_date = df_2.index.min()\n",
    "X = (df_2.index - reference_date).days.values.reshape(-1, 1)\n",
    "y = df_2['JPM UN Equity'].values\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "tests_size = len(X)-train_size\n",
    "X_train, X_test = X[0:train_size],X[train_size:len(X)]\n",
    "y_train, y_test = y[0:train_size],y[train_size:len(X)]\n",
    "\n",
    "# Use auto_arima to find the best ARIMA model\n",
    "model = auto_arima(df_2['JPM UN Equity'], seasonal=True, m=12, suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Get the selected order from auto_arima\n",
    "order = model.get_params()['order']\n",
    "\n",
    "# Train ARIMA model with the selected order\n",
    "arima_model = ARIMA(df_2['JPM UN Equity'], order=order)\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "# Forecast using the trained ARIMA model\n",
    "forecast = arima_fit.forecast(steps=len(X_test))\n",
    "\n",
    "# Evaluate the model using AIC\n",
    "aic = arima_fit.aic\n",
    "\n",
    "y_train_pred = model.predict(n_periods=len(X_train))\n",
    "y_test_pred = model.predict(n_periods=len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774057e5-a28e-4bc6-9fbc-4076e76e17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Selected ARIMA Order: {order}\")\n",
    "print(f\"AIC: {aic}\")\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('stock_price prediction')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('stock_price')\n",
    "plt.xticks(rotation=360, ha='right')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train, y_train, label='Train', alpha=0.3, color='red')\n",
    "plt.scatter(X_test, y_test, label='Test', alpha=0.3, color='green')\n",
    "plt.scatter(X_test, y_test_pred, label='Predicted', alpha=0.3, color='orange')\n",
    "plt.title('Stock Price Prediction with Auto ARIMA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405169e3-9642-485b-8dbd-8b90f12c209b",
   "metadata": {},
   "source": [
    "#### <u>Reviewing of the Original Dataset For BAC</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bd8fa-75c5-46fe-8dd3-86379902a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('stockprice_four.csv')\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106880c-e0fd-401a-b919-e66a9106d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1 = pd.read_csv('stockprice_four.csv')\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c97fd-174b-44a8-bce8-8e350791dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1\n",
    "df_2['Dates'] = pd.to_datetime(df_1['Dates'], format='%d/%m/%Y')\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "\n",
    "# Set 'Dates' as the index of the DataFrame\n",
    "df_2 = df_2.set_index('Dates')\n",
    "if not isinstance(df_2.index, pd.DatetimeIndex):\n",
    "    # If not, set the frequency to 'D'\n",
    "    df_2.index = pd.DatetimeIndex(df_2.index, freq='D')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8b372-91e4-471d-b5f6-650312dab05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null \n",
    "\n",
    "df_2.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a54c74-6d37-46fc-a0e9-8d46f6256161",
   "metadata": {},
   "source": [
    "# Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13251dfb-e517-49ee-be58-b91b81e45884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot trendline with date and stockprice\n",
    "\n",
    "year = df_2.index\n",
    "stock_price = df_2['BAC UN EQUITY']\n",
    "plt.figure(figsize=(25, 8))\n",
    "\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('stock_price prediction')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('stock_price')\n",
    "plt.xticks(rotation=360, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3299be50-4118-4e62-8ec6-5ddcc4b024db",
   "metadata": {},
   "source": [
    "# Augmented Dickey Fuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf845d8-ba3b-4324-ba16-1b4b714483c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ADF test\n",
    "result = adfuller(stock_price)\n",
    "\n",
    "# Extract and print the results\n",
    "adf_statistic = result[0]\n",
    "p_value = result[1]\n",
    "critical_values = result[4]\n",
    "\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "print('Critical Values:')\n",
    "for key, value in critical_values.items():\n",
    "    print(f'   {key}: {value}')\n",
    "\n",
    "# Check for stationarity based on the p-value\n",
    "if p_value <= 0.05:\n",
    "    print(\"Reject the null hypothesis. The data is likely stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. The data may not be stationary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d57af-d926-4d06-8ac7-2932e7fd4f12",
   "metadata": {},
   "source": [
    "# Autocorrelation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf1351-4739-4868-ad53-40d8fcc1d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot ACF\n",
    "plot_acf(stock_price, lags=30, title='Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "plot_pacf(stock_price, lags=30, title='Partial Autocorrelation Function (PACF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a6f7a-4133-4f66-9a42-7224c1993b37",
   "metadata": {},
   "source": [
    "# Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00a446-6529-402b-99a9-912f8bcfff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(data, window, 'same') \n",
    "\n",
    "window_size = 30 # size of moving window\n",
    "\n",
    "ma = moving_average(stock_price, window_size)\n",
    "print(ma)\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.plot(year, ma)\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('Stock Price Moving Average')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6853b1-ec1c-4e50-9959-5cca25623199",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c267a-144f-4336-858e-3fcab73b8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform decomposition\n",
    "decomposition = seasonal_decompose(df_2['BAC UN EQUITY'], model='additive', period=365)\n",
    "\n",
    "\n",
    "# Plot the decomposition components\n",
    "fig, ax = plt.subplots(4, 1, figsize=(10, 8))\n",
    "\n",
    "ax[0].plot(df_2.index, decomposition.observed)\n",
    "ax[0].set_title('Original Series')\n",
    "\n",
    "ax[1].plot(df_2.index, decomposition.trend)\n",
    "ax[1].set_title('Trend')\n",
    "\n",
    "ax[2].plot(df_2.index, decomposition.seasonal)\n",
    "ax[2].set_title('Seasonality')\n",
    "\n",
    "ax[3].plot(df_2.index, decomposition.resid)\n",
    "ax[3].set_title('Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202735b5-fb26-4343-9780-98087c86d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_data, test_data = train_test_split(df_2, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Fit Auto ARIMA model\n",
    "model = auto_arima(train_data['BAC UN EQUITY'], seasonal=True, m=12, suppress_warnings=True)\n",
    "\n",
    "# Make predictions\n",
    "forecast = model.predict(n_periods=len(test_data))\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(test_data['BAC UN EQUITY'], forecast)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f70a4f-711a-454f-934b-c183e186106b",
   "metadata": {},
   "source": [
    "# Auto Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f4863-fa1c-4efb-bdf7-9743edef764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Use auto_arima to find the best model\n",
    "model = auto_arima(df_2['BAC UN EQUITY'], seasonal=True, m=12, suppress_warnings=True)\n",
    "\n",
    "# Get the selected order\n",
    "selected_order = model.order\n",
    "print(\"Selected ARIMA Order:\", selected_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ebe0c-8b82-4796-9575-fd792be792df",
   "metadata": {},
   "source": [
    "# AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7aae50-cbc7-4f1b-906c-4a40904ba7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['BAC UN EQUITY'], label='Original Data', marker='o')\n",
    "plt.title('Original BAC UN EQUITY')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('BAC UN EQUITY')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the 'StockPrice' column\n",
    "df_2['DifferencedBAC UN EQUITY'] = df_2['BAC UN EQUITY'].diff()\n",
    "\n",
    "# Display the differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['DifferencedBAC UN EQUITY'], label='Differenced Data', marker='o', color='orange')\n",
    "plt.title('Differenced BAC UN EQUITY')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Differenced BAC UN EQUITY')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea4e8d-bac7-4f03-8299-26ee168af054",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2['DifferencedBAC UN EQUITY'] #= #df['BARC LN Equity'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557203b-83fc-444d-a651-ab83d1c90ecd",
   "metadata": {},
   "source": [
    "# second order differencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf459fed-350e-4e1d-97e3-95b3c6051265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the file path with the location of your dataset\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['BAC UN EQUITY'], label='Original Data', marker='o')\n",
    "plt.title('Original BAC UN EQUITY')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('BAC UN EQUITY')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the 'StockPrice' column\n",
    "\n",
    "# Display the first differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['DifferencedBAC UN EQUITY'], label='1st Differenced Data', marker='o', color='orange')\n",
    "plt.title('1st Differenced BAC UN EQUITY')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('1st Differenced BAC UN EQUITY')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform differencing on the already differenced data\n",
    "df_2['SecondDifferencedBAC UN EQUITY'] = df_2['DifferencedBAC UN EQUITY'].diff()\n",
    "\n",
    "# Display the second differenced time series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_2.index, df_2['SecondDifferencedBAC UN EQUITY'], label='2nd Differenced Data', marker='o', color='green')\n",
    "plt.title('2nd DifferencedBAC UN EQUITY')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('2nd Differenced BAC UN EQUITY')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05bd7be-8aad-4789-b484-1f997503be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA (AutoRegressive Integrated Moving Average) model\n",
    "\n",
    "# Plot the original time series\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(df_2.index, df_2['BAC UN EQUITY'], label='Original Data')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Stock Price')\n",
    "\n",
    "# Fit ARIMA model\n",
    "order = (0, 1, 0)  # Replace p, d, q with appropriate values\n",
    "model = ARIMA(df_2['BAC UN EQUITY'], order=order)\n",
    "result = model.fit()\n",
    "\n",
    "# Get predictions\n",
    "predictions = result.predict(start=df_2.index.min(), end=df_2.index.max(), dynamic=False)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(24, 8))\n",
    "plt.plot(df_2.index, predictions, color='red', label='ARIMA Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78671b77-a990-43df-af90-1736ba02d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform a grid search for ARIMA parameters using AIC\n",
    "# Define the range of values for p, d, and q\n",
    "p_values = [0, 1, 2]\n",
    "d_values = [0, 1]\n",
    "q_values = [0, 1, 2]\n",
    "\n",
    "# Initialize variables for optimal values and minimum AIC\n",
    "best_aic = float(\"inf\")\n",
    "best_order = None\n",
    "\n",
    "# Perform grid search\n",
    "for p, d, q in itertools.product(p_values, d_values, q_values):\n",
    "    order = (p, d, q)\n",
    "    try:\n",
    "        model = ARIMA(df_2['BAC UN EQUITY'], order=order)\n",
    "        result = model.fit()\n",
    "        aic = result.aic\n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            best_order = order\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"Best AIC: {best_aic}\")\n",
    "print(f\"Best Order (p, d, q): {best_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff17c9-3c01-4f56-b0bc-4d141277cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_date = df_2.index.min()\n",
    "X = (df_2.index - reference_date).days.values.reshape(-1, 1)\n",
    "y = df_2['BAC UN EQUITY'].values\n",
    "\n",
    "\n",
    "\n",
    "train_size = int(len(X)*0.8)\n",
    "tests_size = len(X)-train_size\n",
    "X_train, X_test = X[0:train_size],X[train_size:len(X)]\n",
    "y_train, y_test = y[0:train_size],y[train_size:len(X)]\n",
    "\n",
    "# Use auto_arima to find the best ARIMA model\n",
    "model = auto_arima(df_2['BAC UN EQUITY'], seasonal=True, m=12, suppress_warnings=True, stepwise=True)\n",
    "\n",
    "# Get the selected order from auto_arima\n",
    "order = model.get_params()['order']\n",
    "\n",
    "# Train ARIMA model with the selected order\n",
    "arima_model = ARIMA(df_2['BAC UN EQUITY'], order=order)\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "# Forecast using the trained ARIMA model\n",
    "forecast = arima_fit.forecast(steps=len(X_test))\n",
    "\n",
    "# Evaluate the model using AIC\n",
    "aic = arima_fit.aic\n",
    "\n",
    "y_train_pred = model.predict(n_periods=len(X_train))\n",
    "y_test_pred = model.predict(n_periods=len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fa5f0-3c18-4eb4-b83b-c21cb4d32fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Selected ARIMA Order: {order}\")\n",
    "print(f\"AIC: {aic}\")\n",
    "plt.plot(year, stock_price)\n",
    "plt.title('stock_price prediction')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('stock_price')\n",
    "plt.xticks(rotation=360, ha='right')\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train, y_train, label='Train', alpha=0.3, color='red')\n",
    "plt.scatter(X_test, y_test, label='Test', alpha=0.3, color='green')\n",
    "plt.scatter(X_test, y_test_pred, label='Predicted', alpha=0.3, color='orange')\n",
    "plt.title('Stock Price Prediction with Auto ARIMA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110331e-bd36-496d-bbd2-4de5d5024ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Excel files\n",
    "df = pd.read_excel('UK.US BANKS STOCK PRICES.xlsx', index_col='Dates')\n",
    "\n",
    "\n",
    "# /Select a stock price column to forecast\n",
    "colIndex = 3\n",
    "data = df.iloc[:, colIndex].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41722b14-893a-4ff7-886f-eba8a0b0c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /data normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d69f0f-dd2c-45e3-9d48-91e69d0722ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Create a time series data set\n",
    "def create_dataset(dataset, time_step=30):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        a = dataset[i:(i + time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455576c7-eb58-453f-b220-e52bae296838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/Take the last year's data and use it to make predictions\n",
    "time_step = 260\n",
    "X, Y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "# /Split data set\n",
    "train_size = int(len(X) * 0.8)\n",
    "test_size = len(X) - train_size\n",
    "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
    "Y_train, Y_test = Y[0:train_size], Y[train_size:len(Y)]\n",
    "\n",
    "# LSTM [samples, time steps, features]/Remodel to the format required by LSTM\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa79e1-45b7-41aa-836c-794bea6fdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BILSTM/Construct BILSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True), input_shape=(time_step, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(50)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# /training model\n",
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test), verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af9f30-49a8-49b9-88dd-1ceca9a85c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future_prices(model, last_sequence, n_future_days=260 * 2):\n",
    "    future_prices = []\n",
    "    current_sequence = last_sequence.copy()\n",
    "\n",
    "    for _ in range(n_future_days):\n",
    "        current_sequence = current_sequence.reshape(1, time_step, 1)\n",
    "        future_price = model.predict(current_sequence)\n",
    "        future_prices.append(future_price[0][0])\n",
    "\n",
    "        # /Update sequence\n",
    "        current_sequence = np.append(current_sequence[:, 1:, :], [[future_price[0]]], axis=1)\n",
    "\n",
    "    return scaler.inverse_transform(np.array(future_prices).reshape(-1, 1))\n",
    "\n",
    "\n",
    "# /Use the last time step data as input\n",
    "last_sequence = scaled_data[-time_step:]\n",
    "future_prices = predict_future_prices(model, last_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332189dc-66e7-4266-b8ef-080fb3a9a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Evaluate the model on the test set\n",
    "test_loss = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "test_predictions = model.predict(X_test)\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "Y_test = scaler.inverse_transform(Y_test.reshape(-1, 1))\n",
    "\n",
    "# MSERMSE/Calculate MSE and RMSE\n",
    "mse = mean_squared_error(Y_test, test_predictions)\n",
    "rmse = sqrt(mse)\n",
    "print(f\"Mean Squared Error (MSE,) on Test Set: {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE,) on Test Set: {rmse}\")\n",
    "\n",
    "mae = mean_absolute_error(Y_test, test_predictions)\n",
    "print(f\"Mean Absolute Error (MAE,) on Test Set: {mae}\")\n",
    "\n",
    "mape = np.mean(np.abs((Y_test - test_predictions) / Y_test)) * 100\n",
    "print(f\"Mean Absolute Percentage Error (MAPE,) on Test Set:{mape}%\")\n",
    "\n",
    "ev = explained_variance_score(Y_test, test_predictions)\n",
    "print(f\"Explained Variance Score (EV,) on Test Set: {ev}\")\n",
    "\n",
    "r2 = r2_score(Y_test, test_predictions)\n",
    "print(f\"R-squared (R,) on Test Set: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab2d57-e70e-4265-9e89-e139fa0fd78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Share price data visualization combined with future forecast share price\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('Stock Prices and Future Prediction')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Prices')\n",
    "plt.plot(df.index, df.iloc[:, colIndex], label='Actual Price')\n",
    "plt.plot(pd.date_range(df.index[-1], periods=260*2, freq='B'), future_prices, label='Predicted Future Price', color='red')\n",
    "plt.legend()\n",
    "plt.savefig('stock_prices_future_prediction.png', format='png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4bd99-ce0c-43d0-a8b3-26b66995ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Visualize the actual and predicted prices of the test set\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('Test Set Actual and Predicted Prices')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.plot(Y_test, label='Actual Price')\n",
    "plt.plot(test_predictions, label='Predicted Price', color='red')\n",
    "plt.legend()\n",
    "plt.savefig('Test_Set_Actual_and_Predicted Prices.png', format='png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894fe8e-8ed0-4a85-90e5-c4f70242c8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
